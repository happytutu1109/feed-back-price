{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlt\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数目： 15593\n"
     ]
    }
   ],
   "source": [
    "def data_loading(data_dir):\n",
    "    data, labels, ids= [], [], []\n",
    "    df = pd.read_csv(data_dir)\n",
    "    data_list = df.loc[:,'discourse_text'].values\n",
    "    label_list_ = df.loc[:,'discourse_type'].values\n",
    "    id_list = df.loc[:,'id'].values\n",
    "    type_dict = {'Lead':1,'Position':2,'Claim':3,'Counterclaim':4,'Rebuttal':5,'Evidence':6,'Concluding Statement':7}\n",
    "    label_list = []\n",
    "    for i in label_list_:\n",
    "        label_list.append(type_dict[i])\n",
    "    if len(data_list)!=len(label_list):\n",
    "        return 'length bug'\n",
    "    n = len(data_list)\n",
    "    for data_ in range(n):\n",
    "        sentence_split = data_list[data_].split(\".\")[:-1] if data_list[data_].split('.')[-1] == str('') else data_list[data_].split(\".\")\n",
    "        label_split = [label_list[data_] for i in range(len(sentence_split))]\n",
    "        id_split = [id_list[data_] for i in range(len(sentence_split))]\n",
    "        for j in range(len(sentence_split)):\n",
    "            if sentence_split[j]!=' ':\n",
    "                data.append(sentence_split[j].lower()) \n",
    "                labels.append(label_split[j])\n",
    "                ids.append(id_split[j])\n",
    "    data_article, labels_article = [],[]\n",
    "    data_sentence, labels_sentence = [],[]\n",
    "    for i in range(len(ids)-1):\n",
    "        if ids[i]==ids[i+1]: \n",
    "            data_sentence.append(data[i])\n",
    "            labels_sentence.append(labels[i])\n",
    "        else:\n",
    "            data_article.append(data_sentence)\n",
    "            labels_article.append(labels_sentence)\n",
    "            data_sentence,labels_sentence = [],[]\n",
    "    return data_article , labels_article\n",
    "\n",
    "\n",
    "train_data = data_loading('./train.csv')\n",
    "print('训练集数目：', len(train_data[0]))\n",
    "#print(train_data[0][0],train_data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(train_data[1][i]) for i in range(len(train_data[1]))],bins = 10,rwidth=1, range=(1,80))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article,labels_ = train_data[0],train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [tensor([[  101,  2715,  4286,  2651,  2024,  2467,  2006,  2037,  3042,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2024,  2467,  2006,  2037,  3042,  2062,  2084,  1019,\n",
      "          2847,  1037,  2154,  2053,  2644,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2035,  2027,  2079,  2003,  3793,  2067,  1998,  2830,  1998,\n",
      "          2074,  2031,  2177, 11834,  2015,  2006,  2591,  2865,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2130,  2079,  2009,  2096,  4439,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2024,  2070,  2428,  2919,  8465,  2043,  4933,  6433,\n",
      "          2043,  2009,  3310,  2000,  1037,  3042,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2070,  3056,  2752,  1999,  1996,  2142,  2163,  7221, 11640,\n",
      "          2013,  2465,  4734,  2074,  2138,  1997,  2009,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2043,  2111,  2031, 11640,  1010,  2027,  2113,  2055,  3056,\n",
      "         18726,  2008,  2027,  2031,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 18726,  2066,  9130, 10474, 16021, 23091,  1998, 10245,  7507,\n",
      "          2102,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2061,  2066,  2065,  1037,  2767,  5829,  2185,  1998,  2017,\n",
      "          2215,  2000,  2022,  1999,  3967,  2017,  2064,  2145,  2022,  1999,\n",
      "          3967,  2011, 14739,  6876,  2030,  3793,  7696,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2111,  2467,  2031,  2367,  3971,  2129,  2000, 10639,  2007,\n",
      "          1037,  3042,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 11640,  2031,  2904,  2349,  2000,  2256,  4245,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  4439,  2003,  2028,  1997,  1996,  2126,  2129,  2000,  2131,\n",
      "          2105,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2111,  2467,  2022,  2006,  2037, 11640,  2096,  2725,  2009,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2029,  2064,  3426,  3809,  3471,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2008,  1005,  1055,  2339,  2045,  1005,  1055,  1037,  2518,\n",
      "          2008,  1005,  1055,  2170,  2053,  3793,  2075,  2096,  4439,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2008,  1005,  1055,  1037,  2428,  2590,  2518,  2000,  3342,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2070,  2111,  2145,  2079,  2009,  2138,  2027,  2228,  2009,\n",
      "          1005,  1055,  5236,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2053,  3043,  2054,  2027,  2079,  2027,  2145,  2031,  2000,\n",
      "         15470,  2009,  2138,  2008,  1005,  1055,  1996,  2069,  2126,  2129,\n",
      "          2106,  2002,  3828,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2823,  2006,  1996,  2739,  2045,  2003,  2593,  2019,  4926,\n",
      "          2030,  1037,  5920,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2009,  2453,  9125,  2619,  2025,  2559,  2073,  2027,  1005,\n",
      "          2128,  2183,  2030,  1056, 28394,  2102,  2008,  2619,  2741,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2009,  2593,  4544,  2030,  2331,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2065,  1037,  8075,  2193,  2758,  1045,  1005,  1049,  2183,\n",
      "          2000,  3102,  2017,  1998,  2027,  2113,  2073,  2017,  2444,  2021,\n",
      "          2017,  2123,  1005,  1056,  2113,  1996,  2711,  1005,  1055,  3967,\n",
      "          1010,   102],\n",
      "        [  101,  2029,  2064,  2203,  2039,  2428,  6649,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 11640,  2024,  2986,  2000,  2224,  1998,  2009,  1005,  1055,\n",
      "          2036,  1996,  2190,  2126,  2000,  2272,  2058,  2393,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2065,  2017,  2175,  2083,  1037,  3291,  1998,  2017,  2064,\n",
      "          1005,  1056,  2424,  2393,  2017,  1010,  2467,  2031,  1037,  3042,\n",
      "          2045,  2007,  2017,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2130,  2295, 11640,  2024,  2109,  2471,  2296,  2154,  2004,\n",
      "          2146,  2004,  2017,  1005,  2128,  3647,  2009,  2052,  2272,  2046,\n",
      "          2224,  2065,  2017,  2131,  2046,  4390,   102,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2191,  2469,  2017,  2079,  2025,  2022,  2066,  2023,  3042,\n",
      "          2096,  2017,  1005,  2128,  1999,  1996,  2690,  1997,  4439,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1996,  2739,  2467,  7172,  2043,  2111,  2079,  2242,  5236,\n",
      "          2105,  2008,  7336,  2037, 11640,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), tensor([[  101,  6853,  2323,  2025,  2022,  2583,  2000,  2224, 11640,  2096,\n",
      "          4082,  1037,  4316,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2040,  2109,  2037,  3042,  2096,  4082,  1037,  4316,\n",
      "          2024,  2087,  3497,  2000,  2131,  2046,  2019,  4926,  2008,  2071,\n",
      "          2022, 10611,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  2019,  3720,  2011,  1996,  9586, 17840,  3813,\n",
      "          1010,  2538,  1003,  1997, 13496,  2008,  2020,  2112,  1997,  1037,\n",
      "         10611,  2482,  4926,  2001,  2349,  2000, 11640,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  1996,  2168,  3720,  1010,  3486,  1003,  2113,\n",
      "          1996,  3891,  2021,  3613,  2478,  2037, 11640,  2096,  2006,  1996,\n",
      "          2346,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2023,  3065,  2008,  2049,  3458,  4795,  1998, 20868,  6072,\n",
      "         26029, 19307,  1997,  6853,  2025,  2000,  2022,  3929,  5204,  1997,\n",
      "          2037, 11301,  2096,  4439,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2323,  2022,  2583,  2000, 10152,  2302,  2151, 14836,\n",
      "          2015,  1010,  2138,  2009,  2071,  2022, 10611,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  2178,  3720,  1010,  1000, 11116,  4439,  1000,\n",
      "          2011,  1996, 18699, 27110,  1010,  2045,  2038,  2525,  2042,  2055,\n",
      "          1017,  1010,  2199,  3042,  3141,  2482,  4926,  6677,  2144,  2418,\n",
      "           102,     0],\n",
      "        [  101,  1996,  3720,  2163,  2008,  9458,  2131,  2205, 11116,  2007,\n",
      "          2037, 11640,  1010,  2029,  5320,  2037,  4926,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 13436,  2008,  2064,  2022,  4089,  9511,  2011,  7995,  2006,\n",
      "          1996,  2346,  1998,  2025,  1037,  3042,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2323,  2025,  2022,  2583,  2000,  2224,  2037, 11640,\n",
      "          2012,  2035,  2096,  4439,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1999,  7091,  1010,  6853,  2323,  2025,  2583,  2000,  2147,\n",
      "          1037,  4316,  2096,  2478,  2037,  3526,  3042,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2040,  3594,  2037, 11640,  2096,  4082,  1037,  4316,\n",
      "          1998,  2024,  3497,  2000,  2031,  2019,  4926,  2059,  2216,  2040,\n",
      "          2123,  1005,  1056,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])] [tensor([1, 1, 1, 1, 2, 6, 6, 6, 6, 6, 6, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3,\n",
      "        6, 6, 6, 7]), tensor([2, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7])]\n",
      "[tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])]\n",
      "[tensor([1, 1, 1, 2, 3, 6, 6, 6, 4, 4, 5, 5, 7]), tensor([1, 1, 1, 2, 3, 3, 3, 3, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7])]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_art = []\n",
    "attention_masks_art = []\n",
    "labels = []\n",
    "# For every sentence...\n",
    "for art in range(len(article)):\n",
    "    input_ids_sent = []\n",
    "    attention_masks_sent = []\n",
    "    for sent in article[art]:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 32,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids_sent.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_sent.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "\n",
    "    input_ids_sent = torch.cat(input_ids_sent, dim=0)\n",
    "    attention_masks_sent = torch.cat(attention_masks_sent, dim=0)\n",
    "    labels_sent = torch.tensor(labels_[art])\n",
    "\n",
    "    input_ids_art.append(input_ids_sent)\n",
    "    attention_masks_art.append(attention_masks_sent)\n",
    "    labels.append(labels_sent)\n",
    "\n",
    "#labels = [label.tolist() for label in labels]\n",
    "\n",
    "#print('Original: ', article[0])\n",
    "print('Token IDs:', input_ids_art[:2],labels[:2])\n",
    "print(attention_masks_art[:2])\n",
    "print(labels[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对文章层面进行padding，并记录文章有效句子个数\n",
    "def padding(id,mask,label,max_sent_num=50,max_sent_len=32):     #确保输入的lst为list列表结构，其中每一个元素为pytorchtensor。max_sent_len需要与前面tokenizer里参数对应上\n",
    "    valid_num = []\n",
    "    for i in range(len(id)):\n",
    "        id[i] = id[i].tolist()\n",
    "        mask[i] = mask[i].tolist()\n",
    "        label[i] = label[i].tolist()\n",
    "        valid_num.append(len(id[i]) if len(id[i])<=50 else 50)\n",
    "        if len(id[i])<max_sent_num:\n",
    "            id[i] = id[i]+[[101,102]+[0 for _ in range(max_sent_len-2)] for __ in range(max_sent_num-len(id[i]))]\n",
    "            mask[i] = mask[i]+[[1,1]+[0 for _ in range(max_sent_len-2)] for __ in range(max_sent_num-len(mask[i]))]\n",
    "            label[i] = label[i] + [0 for _ in range(max_sent_num-len(label[i]))]\n",
    "        if len(id[i])>max_sent_num:\n",
    "            id[i] = id[i][:20]+id[i][-30:]\n",
    "            mask[i] = mask[i][:20]+id[i][-30:]\n",
    "            label[i] = label[i][:20]+label[i][-30:]\n",
    "    #return id,mask,valid_num\n",
    "    return torch.tensor(id),torch.tensor(mask),torch.tensor(label),valid_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_art,attention_masks_art,labels,valid_num = padding(input_ids_art,attention_masks_art,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15593, 50, 32]),\n",
       " torch.Size([15593, 50, 32]),\n",
       " torch.Size([15593, 50]),\n",
       " 15593)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_art.size(),attention_masks_art.size(),labels.size(),len(valid_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class article_dataset(Dataset):\n",
    "    def __init__(self,input,masks,labels,valid_num):\n",
    "        super(article_dataset,self).__init__()\n",
    "        self.input = input\n",
    "        self.labels = labels\n",
    "        self.masks = masks\n",
    "        self.valid_num = valid_num\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input[idx],self.masks[idx],self.labels[idx],self.valid_num[idx]      #input和masks维度为sentence_num,sentence_len，label维度 sentence_num\n",
    "    def __len__(self):                                                                 #valid_num为int\n",
    "        return len(self.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章中句子个数不一致问题暂定解决办法：\n",
    "    1：找到合适的sentence_num（尽量所有文章的长度均小于该值）。\n",
    "    2：对小于该长度的文章，在处理完tokenizer之前进行空句子补齐，并记录其有效长度。（其会在tokennizer阶段被标注为[101,102,...,0]）。\n",
    "    3：所有用于补齐的空句子不设置类别，在处理空句子时记录每篇文章的有效句子长度。句子有效长度在每一个batch中使用一个list储存（想办法在dataset中实现）\n",
    "    4：在模型框架中，仅输出有效句子的损失并进行后向传播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = article_dataset(input_ids_art,attention_masks_art,labels,valid_num)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=2,drop_last=True)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset,sampler=val_sampler,batch_size=2,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  101,  3765,  2031,  ...,     0,     0,     0],\n",
      "         [  101,  2174,  1010,  ...,     0,     0,     0],\n",
      "         [  101,  2057,  2123,  ...,     0,     0,     0],\n",
      "         ...,\n",
      "         [  101,   102,     0,  ...,     0,     0,     0],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0]],\n",
      "\n",
      "        [[  101, 20024,  1010,  ...,     0,     0,     0],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0],\n",
      "         [  101,  1015,  2041,  ...,     0,     0,     0],\n",
      "         ...,\n",
      "         [  101,   102,     0,  ...,     0,     0,     0],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0],\n",
      "         [  101,   102,     0,  ...,     0,     0,     0]]]) tensor([[[1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0]]]) tensor([[1, 3, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 3, 7, 7, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 6, 6, 6, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]) tensor([20, 24])\n",
      "len(train_dataloader)= 7016\n"
     ]
    }
   ],
   "source": [
    "for i, b in enumerate(train_dataloader):\n",
    "    print (b[0],b[1],b[2],b[3])\n",
    "    break\n",
    "print('len(train_dataloader)=', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self,bert_name,hidden_size_rnn,num_layers_rnn,classes,dropout):     #valid_num表示该input中有效的sentence个数\n",
    "        super(HAN,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_name)\n",
    "        hidden_size_bert = self.bert.config.hidden_size\n",
    "        self.rnn = nn.LSTM(input_size=hidden_size_bert, hidden_size=hidden_size_rnn, num_layers=num_layers_rnn, bias=True, dropout=dropout,batch_first=True)\n",
    "        self.outlayer = nn.Linear(hidden_size_rnn,classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.classes = classes\n",
    "        \n",
    "    def forward(self,input_ids,input_masks,valid_num,labels=None):  #input维度 batchsize,num_sentence,sent_len , label维度batchsize,num_sentence,sent_len\n",
    "        batch_size,sentence_num,sentence_len = input_ids.size(0),input_ids.size(1),input_ids.size(2)\n",
    "        hidden_size_bert = self.bert.config.hidden_size\n",
    "        input_ids,input_masks = input_ids.view(-1,sentence_len),input_masks.view(-1,sentence_len)\n",
    "        out = self.bert(input_ids,input_masks)      #out维度 batchsize*num_sentence,sen_len,hidden_size\n",
    "        out = out[0][:,0,:]     #out维度 batchsize*num_sentence,hidden_size\n",
    "        out = out.view(batch_size,sentence_num,hidden_size_bert)    #out维度转化为batchsize,num_sentence,hidden_size\n",
    "        self.rnn.flatten_parameters()\n",
    "        outputs,_ = self.rnn(out)   #outputs维度 batchsize,num_sentence,hidden_size\n",
    "        outputs = self.outlayer(outputs)    #outputs维度为 batchsize,num_sentence,classes\n",
    "        output = []\n",
    "        for batch in range(len(outputs)):\n",
    "            output.append(outputs[batch][0:valid_num[batch],:].tolist())\n",
    "        if labels is not None:\n",
    "            outputs_loss,labels = outputs.view(-1,self.classes),labels.view(-1)\n",
    "            #outputs = torch.softmax(outputs)\n",
    "            loss = self.loss(outputs_loss,labels)\n",
    "            return loss,outputs\n",
    "        else:\n",
    "            return 0,outputs\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HAN(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (rnn): LSTM(768, 256, batch_first=True, dropout=0.1)\n",
       "  (outlayer): Linear(in_features=256, out_features=8, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HAN('bert-base-uncased',256,1,8,0.1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "epochs = 2\n",
    "# training steps 的数量: [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# 设计 learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_acc(preds, labels,classes):     \n",
    "    preds = preds.view(-1,classes)\n",
    "    labels = labels.view(-1)\n",
    "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float()      #eq里面的两个参数的shape=torch.Size([16])    \n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc_eva(preds, labels,classes,valid,batchsize=2):     #仅限batchsize为2时使用\n",
    "    \n",
    "    preds = torch.cat([preds[0][:valid[0]],preds[1][:valid[1]]],dim=0)\n",
    "\n",
    "    labels = torch.cat([labels[0][:valid[0]],labels[1][:valid[1]]],dim=0)\n",
    "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float()      #eq里面的两个参数的shape=torch.Size([16])    \n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):    \n",
    "    elapsed_rounded = int(round((elapsed)))    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer):\n",
    "    t0 = time.time()\n",
    "    avg_loss, avg_acc = [],[]   \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 每隔40个batch 输出一下所用时间.\n",
    "        if step % 40 == 0 and not step == 0:            \n",
    "            elapsed = format_time(time.time() - t0)             \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print(np.array(avg_loss).mean())\n",
    "        input_id,input_mask,label,valid_num_tensor = batch[0].long().to(device),batch[1].long().to(device),batch[2].long().to(device),batch[3]\n",
    "        valid_num = valid_num_tensor.tolist()\n",
    "        output = model(input_id,input_mask,valid_num,label)\n",
    "        loss,outputs = output[0],output[1]\n",
    "        avg_loss.append(loss.item())\n",
    "       \n",
    "        acc = binary_acc(outputs, label,classes=8)\n",
    "        avg_acc.append(acc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 1.0)      #大于1的梯度将其设为1.0, 以防梯度爆炸\n",
    "        optimizer.step()              #更新模型参数\n",
    "        scheduler.step()              #更新learning rate\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    return avg_loss, avg_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  7,016.    Elapsed: 0:00:10.\n",
      "1.0182715743780135\n",
      "  Batch    80  of  7,016.    Elapsed: 0:00:20.\n",
      "0.8456091985106469\n",
      "  Batch   120  of  7,016.    Elapsed: 0:00:29.\n",
      "0.759788592159748\n",
      "  Batch   160  of  7,016.    Elapsed: 0:00:39.\n",
      "0.7230642061680556\n",
      "  Batch   200  of  7,016.    Elapsed: 0:00:48.\n",
      "0.6845995192974805\n",
      "  Batch   240  of  7,016.    Elapsed: 0:00:58.\n",
      "0.6649044804275036\n",
      "  Batch   280  of  7,016.    Elapsed: 0:01:07.\n",
      "0.6409908095906888\n",
      "  Batch   320  of  7,016.    Elapsed: 0:01:17.\n",
      "0.6271645904751495\n",
      "  Batch   360  of  7,016.    Elapsed: 0:01:26.\n",
      "0.6167653190799885\n",
      "  Batch   400  of  7,016.    Elapsed: 0:01:36.\n",
      "0.6017684032954276\n",
      "  Batch   440  of  7,016.    Elapsed: 0:01:45.\n",
      "0.5925266598605297\n",
      "  Batch   480  of  7,016.    Elapsed: 0:01:54.\n",
      "0.5833698952415337\n",
      "  Batch   520  of  7,016.    Elapsed: 0:02:02.\n",
      "0.5710639658598946\n",
      "  Batch   560  of  7,016.    Elapsed: 0:02:10.\n",
      "0.5597193800844252\n",
      "  Batch   600  of  7,016.    Elapsed: 0:02:18.\n",
      "0.5520967103665074\n",
      "  Batch   640  of  7,016.    Elapsed: 0:02:26.\n",
      "0.5447231805068441\n",
      "  Batch   680  of  7,016.    Elapsed: 0:02:34.\n",
      "0.5344750106005984\n",
      "  Batch   720  of  7,016.    Elapsed: 0:02:43.\n",
      "0.5270416533677942\n",
      "  Batch   760  of  7,016.    Elapsed: 0:02:51.\n",
      "0.5232653810103473\n",
      "  Batch   800  of  7,016.    Elapsed: 0:02:59.\n",
      "0.520328912185505\n",
      "  Batch   840  of  7,016.    Elapsed: 0:03:07.\n",
      "0.5155308378860355\n",
      "  Batch   880  of  7,016.    Elapsed: 0:03:16.\n",
      "0.5102702794173224\n",
      "  Batch   920  of  7,016.    Elapsed: 0:03:25.\n",
      "0.5029669607544075\n",
      "  Batch   960  of  7,016.    Elapsed: 0:03:33.\n",
      "0.4991108107147738\n",
      "  Batch 1,000  of  7,016.    Elapsed: 0:03:41.\n",
      "0.4945174228921533\n",
      "  Batch 1,040  of  7,016.    Elapsed: 0:03:50.\n",
      "0.49221328215387006\n",
      "  Batch 1,080  of  7,016.    Elapsed: 0:03:58.\n",
      "0.48846503356126725\n",
      "  Batch 1,120  of  7,016.    Elapsed: 0:04:06.\n",
      "0.4835601253634585\n",
      "  Batch 1,160  of  7,016.    Elapsed: 0:04:15.\n",
      "0.4803877307744376\n",
      "  Batch 1,200  of  7,016.    Elapsed: 0:04:23.\n",
      "0.47880514747773606\n",
      "  Batch 1,240  of  7,016.    Elapsed: 0:04:32.\n",
      "0.47676712516695263\n",
      "  Batch 1,280  of  7,016.    Elapsed: 0:04:40.\n",
      "0.47297229208634234\n",
      "  Batch 1,320  of  7,016.    Elapsed: 0:04:49.\n",
      "0.4700948561626402\n",
      "  Batch 1,360  of  7,016.    Elapsed: 0:04:57.\n",
      "0.46635851875933654\n",
      "  Batch 1,400  of  7,016.    Elapsed: 0:05:05.\n",
      "0.4640086712528552\n",
      "  Batch 1,440  of  7,016.    Elapsed: 0:05:13.\n",
      "0.46082017935502034\n",
      "  Batch 1,480  of  7,016.    Elapsed: 0:05:22.\n",
      "0.4581431825670439\n",
      "  Batch 1,520  of  7,016.    Elapsed: 0:05:30.\n",
      "0.4563109528616463\n",
      "  Batch 1,560  of  7,016.    Elapsed: 0:05:38.\n",
      "0.45499253743447554\n",
      "  Batch 1,600  of  7,016.    Elapsed: 0:05:46.\n",
      "0.45356428374070673\n",
      "  Batch 1,640  of  7,016.    Elapsed: 0:05:55.\n",
      "0.451069566939117\n",
      "  Batch 1,680  of  7,016.    Elapsed: 0:06:03.\n",
      "0.44739451556067383\n",
      "  Batch 1,720  of  7,016.    Elapsed: 0:06:11.\n",
      "0.4446272701588134\n",
      "  Batch 1,760  of  7,016.    Elapsed: 0:06:20.\n",
      "0.44262545375915413\n",
      "  Batch 1,800  of  7,016.    Elapsed: 0:06:28.\n",
      "0.44063984045965804\n",
      "  Batch 1,840  of  7,016.    Elapsed: 0:06:36.\n",
      "0.438829256841184\n",
      "  Batch 1,880  of  7,016.    Elapsed: 0:06:45.\n",
      "0.43686448017412677\n",
      "  Batch 1,920  of  7,016.    Elapsed: 0:06:53.\n",
      "0.4354366655422685\n",
      "  Batch 1,960  of  7,016.    Elapsed: 0:07:01.\n",
      "0.4334506120684804\n",
      "  Batch 2,000  of  7,016.    Elapsed: 0:07:10.\n",
      "0.4319048174098134\n",
      "  Batch 2,040  of  7,016.    Elapsed: 0:07:19.\n",
      "0.4306397471942154\n",
      "  Batch 2,080  of  7,016.    Elapsed: 0:07:27.\n",
      "0.4294352368666576\n",
      "  Batch 2,120  of  7,016.    Elapsed: 0:07:35.\n",
      "0.42699361891960197\n",
      "  Batch 2,160  of  7,016.    Elapsed: 0:07:43.\n",
      "0.4247724388208654\n",
      "  Batch 2,200  of  7,016.    Elapsed: 0:07:52.\n",
      "0.42286836888302454\n",
      "  Batch 2,240  of  7,016.    Elapsed: 0:08:00.\n",
      "0.42217274491808243\n",
      "  Batch 2,280  of  7,016.    Elapsed: 0:08:08.\n",
      "0.42018292853118555\n",
      "  Batch 2,320  of  7,016.    Elapsed: 0:08:17.\n",
      "0.41879805281822535\n",
      "  Batch 2,360  of  7,016.    Elapsed: 0:08:26.\n",
      "0.41653168137990315\n",
      "  Batch 2,400  of  7,016.    Elapsed: 0:08:35.\n",
      "0.41506057855673134\n",
      "  Batch 2,440  of  7,016.    Elapsed: 0:08:43.\n",
      "0.41391107503507957\n",
      "  Batch 2,480  of  7,016.    Elapsed: 0:08:51.\n",
      "0.41241527689741025\n",
      "  Batch 2,520  of  7,016.    Elapsed: 0:08:59.\n",
      "0.4110659601846858\n",
      "  Batch 2,560  of  7,016.    Elapsed: 0:09:07.\n",
      "0.41010706371453126\n",
      "  Batch 2,600  of  7,016.    Elapsed: 0:09:15.\n",
      "0.40895660856881966\n",
      "  Batch 2,640  of  7,016.    Elapsed: 0:09:23.\n",
      "0.4074381611846162\n",
      "  Batch 2,680  of  7,016.    Elapsed: 0:09:31.\n",
      "0.40625520910678514\n",
      "  Batch 2,720  of  7,016.    Elapsed: 0:09:39.\n",
      "0.4052297967869569\n",
      "  Batch 2,760  of  7,016.    Elapsed: 0:09:49.\n",
      "0.40430771569195\n",
      "  Batch 2,800  of  7,016.    Elapsed: 0:09:59.\n",
      "0.4027228975748377\n",
      "  Batch 2,840  of  7,016.    Elapsed: 0:10:09.\n",
      "0.4014254134279531\n",
      "  Batch 2,880  of  7,016.    Elapsed: 0:10:19.\n",
      "0.39992430151905867\n",
      "  Batch 2,920  of  7,016.    Elapsed: 0:10:29.\n",
      "0.3986296531527418\n",
      "  Batch 2,960  of  7,016.    Elapsed: 0:10:39.\n",
      "0.39818615771930765\n",
      "  Batch 3,000  of  7,016.    Elapsed: 0:10:50.\n",
      "0.397270848557353\n",
      "  Batch 3,040  of  7,016.    Elapsed: 0:11:00.\n",
      "0.3958071059964009\n",
      "  Batch 3,080  of  7,016.    Elapsed: 0:11:10.\n",
      "0.3946297120569008\n",
      "  Batch 3,120  of  7,016.    Elapsed: 0:11:20.\n",
      "0.3936411131030092\n",
      "  Batch 3,160  of  7,016.    Elapsed: 0:11:30.\n",
      "0.3931544229906948\n",
      "  Batch 3,200  of  7,016.    Elapsed: 0:11:40.\n",
      "0.39213211082387717\n",
      "  Batch 3,240  of  7,016.    Elapsed: 0:11:50.\n",
      "0.3916742452684744\n",
      "  Batch 3,280  of  7,016.    Elapsed: 0:12:00.\n",
      "0.3906273142684524\n",
      "  Batch 3,320  of  7,016.    Elapsed: 0:12:09.\n",
      "0.38960223938731187\n",
      "  Batch 3,360  of  7,016.    Elapsed: 0:12:18.\n",
      "0.3884306248055682\n",
      "  Batch 3,400  of  7,016.    Elapsed: 0:12:27.\n",
      "0.38759552053011515\n",
      "  Batch 3,440  of  7,016.    Elapsed: 0:12:36.\n",
      "0.38629900587636024\n",
      "  Batch 3,480  of  7,016.    Elapsed: 0:12:45.\n",
      "0.3856349704722906\n",
      "  Batch 3,520  of  7,016.    Elapsed: 0:12:53.\n",
      "0.3850841188697483\n",
      "  Batch 3,560  of  7,016.    Elapsed: 0:13:01.\n",
      "0.3840509811918555\n",
      "  Batch 3,600  of  7,016.    Elapsed: 0:13:10.\n",
      "0.38284835954300234\n",
      "  Batch 3,640  of  7,016.    Elapsed: 0:13:18.\n",
      "0.38235383324611644\n",
      "  Batch 3,680  of  7,016.    Elapsed: 0:13:27.\n",
      "0.38171418694900755\n",
      "  Batch 3,720  of  7,016.    Elapsed: 0:13:35.\n",
      "0.38146492198149684\n",
      "  Batch 3,760  of  7,016.    Elapsed: 0:13:44.\n",
      "0.3809302881895069\n",
      "  Batch 3,800  of  7,016.    Elapsed: 0:13:52.\n",
      "0.38034709068701456\n",
      "  Batch 3,840  of  7,016.    Elapsed: 0:14:01.\n",
      "0.3795359205008329\n",
      "  Batch 3,880  of  7,016.    Elapsed: 0:14:09.\n",
      "0.3787858736154038\n",
      "  Batch 3,920  of  7,016.    Elapsed: 0:14:18.\n",
      "0.3787418146826783\n",
      "  Batch 3,960  of  7,016.    Elapsed: 0:14:26.\n",
      "0.37794723774146555\n",
      "  Batch 4,000  of  7,016.    Elapsed: 0:14:34.\n",
      "0.37730461232364176\n",
      "  Batch 4,040  of  7,016.    Elapsed: 0:14:43.\n",
      "0.3766593416617944\n",
      "  Batch 4,080  of  7,016.    Elapsed: 0:14:52.\n",
      "0.3761298427924368\n",
      "  Batch 4,120  of  7,016.    Elapsed: 0:15:02.\n",
      "0.37568303960583455\n",
      "  Batch 4,160  of  7,016.    Elapsed: 0:15:12.\n",
      "0.3752923390040031\n",
      "  Batch 4,200  of  7,016.    Elapsed: 0:15:21.\n",
      "0.3748707194076408\n",
      "  Batch 4,240  of  7,016.    Elapsed: 0:15:29.\n",
      "0.3744414809247795\n",
      "  Batch 4,280  of  7,016.    Elapsed: 0:15:38.\n",
      "0.3740899226584724\n",
      "  Batch 4,320  of  7,016.    Elapsed: 0:15:47.\n",
      "0.37331188292139106\n",
      "  Batch 4,360  of  7,016.    Elapsed: 0:15:55.\n",
      "0.3730765620161087\n",
      "  Batch 4,400  of  7,016.    Elapsed: 0:16:04.\n",
      "0.37202361443334003\n",
      "  Batch 4,440  of  7,016.    Elapsed: 0:16:13.\n",
      "0.3715056673153765\n",
      "  Batch 4,480  of  7,016.    Elapsed: 0:16:22.\n",
      "0.37084201073656525\n",
      "  Batch 4,520  of  7,016.    Elapsed: 0:16:30.\n",
      "0.37034802963067076\n",
      "  Batch 4,560  of  7,016.    Elapsed: 0:16:39.\n",
      "0.3696238510759972\n",
      "  Batch 4,600  of  7,016.    Elapsed: 0:16:47.\n",
      "0.36899006668235296\n",
      "  Batch 4,640  of  7,016.    Elapsed: 0:16:56.\n",
      "0.368687849081571\n",
      "  Batch 4,680  of  7,016.    Elapsed: 0:17:04.\n",
      "0.36805847818350307\n",
      "  Batch 4,720  of  7,016.    Elapsed: 0:17:13.\n",
      "0.3675400992894892\n",
      "  Batch 4,760  of  7,016.    Elapsed: 0:17:21.\n",
      "0.3669907974822744\n",
      "  Batch 4,800  of  7,016.    Elapsed: 0:17:30.\n",
      "0.3665985096921213\n",
      "  Batch 4,840  of  7,016.    Elapsed: 0:17:38.\n",
      "0.3663573622280224\n",
      "  Batch 4,880  of  7,016.    Elapsed: 0:17:47.\n",
      "0.36588027589404803\n",
      "  Batch 4,920  of  7,016.    Elapsed: 0:17:56.\n",
      "0.36531793830340836\n",
      "  Batch 4,960  of  7,016.    Elapsed: 0:18:04.\n",
      "0.36471880194055095\n",
      "  Batch 5,000  of  7,016.    Elapsed: 0:18:12.\n",
      "0.3638974442385137\n",
      "  Batch 5,040  of  7,016.    Elapsed: 0:18:21.\n",
      "0.36347991195655177\n",
      "  Batch 5,080  of  7,016.    Elapsed: 0:18:29.\n",
      "0.3631851289208656\n",
      "  Batch 5,120  of  7,016.    Elapsed: 0:18:38.\n",
      "0.3625544959380932\n",
      "  Batch 5,160  of  7,016.    Elapsed: 0:18:47.\n",
      "0.3625122145504164\n",
      "  Batch 5,200  of  7,016.    Elapsed: 0:18:55.\n",
      "0.3619836665553829\n",
      "  Batch 5,240  of  7,016.    Elapsed: 0:19:04.\n",
      "0.3613205763312298\n",
      "  Batch 5,280  of  7,016.    Elapsed: 0:19:13.\n",
      "0.3610299175132461\n",
      "  Batch 5,320  of  7,016.    Elapsed: 0:19:22.\n",
      "0.3606159410873582\n",
      "  Batch 5,360  of  7,016.    Elapsed: 0:19:31.\n",
      "0.36001004521846214\n",
      "  Batch 5,400  of  7,016.    Elapsed: 0:19:39.\n",
      "0.35931767436296297\n",
      "  Batch 5,440  of  7,016.    Elapsed: 0:19:48.\n",
      "0.3589038713143536\n",
      "  Batch 5,480  of  7,016.    Elapsed: 0:19:56.\n",
      "0.3585767151794675\n",
      "  Batch 5,520  of  7,016.    Elapsed: 0:20:04.\n",
      "0.3582136641182275\n",
      "  Batch 5,560  of  7,016.    Elapsed: 0:20:12.\n",
      "0.3576807080526843\n",
      "  Batch 5,600  of  7,016.    Elapsed: 0:20:20.\n",
      "0.35772573750000447\n",
      "  Batch 5,640  of  7,016.    Elapsed: 0:20:29.\n",
      "0.35714636694256824\n",
      "  Batch 5,680  of  7,016.    Elapsed: 0:20:37.\n",
      "0.35677953803400236\n",
      "  Batch 5,720  of  7,016.    Elapsed: 0:20:47.\n",
      "0.35656132742801644\n",
      "  Batch 5,760  of  7,016.    Elapsed: 0:20:57.\n",
      "0.35624058865877384\n",
      "  Batch 5,800  of  7,016.    Elapsed: 0:21:07.\n",
      "0.35589738914357694\n",
      "  Batch 5,840  of  7,016.    Elapsed: 0:21:17.\n",
      "0.35533424734826874\n",
      "  Batch 5,880  of  7,016.    Elapsed: 0:21:27.\n",
      "0.35501149880452726\n",
      "  Batch 5,920  of  7,016.    Elapsed: 0:21:36.\n",
      "0.3544056453798721\n",
      "  Batch 5,960  of  7,016.    Elapsed: 0:21:46.\n",
      "0.3540190883849641\n",
      "  Batch 6,000  of  7,016.    Elapsed: 0:21:55.\n",
      "0.35370535392003755\n",
      "  Batch 6,040  of  7,016.    Elapsed: 0:22:04.\n",
      "0.3536150724799773\n",
      "  Batch 6,080  of  7,016.    Elapsed: 0:22:12.\n",
      "0.35320218978756057\n",
      "  Batch 6,120  of  7,016.    Elapsed: 0:22:21.\n",
      "0.35277815871610263\n",
      "  Batch 6,160  of  7,016.    Elapsed: 0:22:29.\n",
      "0.35242967197199826\n",
      "  Batch 6,200  of  7,016.    Elapsed: 0:22:37.\n",
      "0.35188794928152234\n",
      "  Batch 6,240  of  7,016.    Elapsed: 0:22:45.\n",
      "0.3515761511836153\n",
      "  Batch 6,280  of  7,016.    Elapsed: 0:22:53.\n",
      "0.3511764037244876\n",
      "  Batch 6,320  of  7,016.    Elapsed: 0:23:03.\n",
      "0.3507386116894504\n",
      "  Batch 6,360  of  7,016.    Elapsed: 0:23:13.\n",
      "0.3502764084505257\n",
      "  Batch 6,400  of  7,016.    Elapsed: 0:23:23.\n",
      "0.34996707779297137\n",
      "  Batch 6,440  of  7,016.    Elapsed: 0:23:33.\n",
      "0.34970400653929906\n",
      "  Batch 6,480  of  7,016.    Elapsed: 0:23:43.\n",
      "0.34935560511873553\n",
      "  Batch 6,520  of  7,016.    Elapsed: 0:23:53.\n",
      "0.34887961141959384\n",
      "  Batch 6,560  of  7,016.    Elapsed: 0:24:03.\n",
      "0.34867238312130566\n",
      "  Batch 6,600  of  7,016.    Elapsed: 0:24:13.\n",
      "0.34824379552212176\n",
      "  Batch 6,640  of  7,016.    Elapsed: 0:24:23.\n",
      "0.3477774828173085\n",
      "  Batch 6,680  of  7,016.    Elapsed: 0:24:33.\n",
      "0.3476079938269616\n",
      "  Batch 6,720  of  7,016.    Elapsed: 0:24:43.\n",
      "0.34695082704946845\n",
      "  Batch 6,760  of  7,016.    Elapsed: 0:24:53.\n",
      "0.34640368380853853\n",
      "  Batch 6,800  of  7,016.    Elapsed: 0:25:03.\n",
      "0.3461051838171175\n",
      "  Batch 6,840  of  7,016.    Elapsed: 0:25:13.\n",
      "0.3458840734020355\n",
      "  Batch 6,880  of  7,016.    Elapsed: 0:25:22.\n",
      "0.34561605163427545\n",
      "  Batch 6,920  of  7,016.    Elapsed: 0:25:32.\n",
      "0.3453843559913682\n",
      "  Batch 6,960  of  7,016.    Elapsed: 0:25:42.\n",
      "0.3451980948110978\n",
      "  Batch 7,000  of  7,016.    Elapsed: 0:25:53.\n",
      "0.3450139739933823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3448176458876202, 0.8814851767388826)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):    \n",
    "    avg_acc = []    \n",
    "    model.eval()         #表示进入测试模式\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_id,input_mask,label,valid_num_tensor = batch[0].long().to(device),batch[1].long().to(device),batch[2].long().to(device),batch[3]\n",
    "            valid_num = valid_num_tensor.tolist()\n",
    "            output = model(input_id,input_mask,valid_num)\n",
    "            acc = binary_acc_eva(output[1], label,8,valid_num)\n",
    "            avg_acc.append(acc)\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1,测试准确率=0.7605332785478492\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate(model)\n",
    "print(\"epoch={},测试准确率={}\".format(1, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = torch.tensor([[[  101,  2715,  4286,  2651,  2024,  2467,  2006,  2037,  3042,   102,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2027,  2024,  2467,  2006,  2037,  3042,  2062,  2084,  1019,\n",
    "          2847,  1037,  2154,  2053,  2644,   102,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]],\n",
    "     [[  101,  2035,  2027,  2079,  2003,  3793,  2067,  1998,  2830,  1998,\n",
    "          2074,  2031,  2177, 11834,  2015,  2006,  2591,  2865,   102,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2027,  2130,  2079,  2009,  2096,  4439,   102,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2027,  2024,  2070,  2428,  2919,  8465,  2043,  4933,  6433,\n",
    "          2043,  2009,  3310,  2000,  1037,  3042,   102,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "          [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]],\n",
    "     [[  101,  2070,  3056,  2752,  1999,  1996,  2142,  2163,  7221, 11640,\n",
    "          2013,  2465,  4734,  2074,  2138,  1997,  2009,   102,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2043,  2111,  2031, 11640,  1010,  2027,  2113,  2055,  3056,\n",
    "         18726,  2008,  2027,  2031,   102,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101, 18726,  2066,  9130, 10474, 16021, 23091,  1998, 10245,  7507,\n",
    "          2102,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2061,  2066,  2065,  1037,  2767,  5829,  2185,  1998,  2017,\n",
    "          2215,  2000,  2022,  1999,  3967,  2017,  2064,  2145,  2022,  1999,\n",
    "          3967,  2011, 14739,  6876,  2030,  3793,  7696,   102,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2111,  2467,  2031,  2367,  3971,  2129,  2000, 10639,  2007,\n",
    "          1037,  3042,   102,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nq = sentence.long().to(device)\\nw = mask.long().to(device)\\nr = torch.tensor([2,3,5])\\ne = torch.tensor([[2,3,0,0,0,0],[5,3,2,0,0,0],[5,5,5,2,6,0]]).long().to(device)\\noutput = model(q,w,r,e)\\noutput\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "q = sentence.long().to(device)\n",
    "w = mask.long().to(device)\n",
    "r = torch.tensor([2,3,5])\n",
    "e = torch.tensor([[2,3,0,0,0,0],[5,3,2,0,0,0],[5,5,5,2,6,0]]).long().to(device)\n",
    "output = model(q,w,r,e)\n",
    "output\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
