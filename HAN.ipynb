{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数目： 534\n"
     ]
    }
   ],
   "source": [
    "def data_loading(data_dir):\n",
    "    data, labels, ids= [], [], []\n",
    "    df = pd.read_csv(data_dir).loc[:5000]\n",
    "    data_list = df.loc[:,'discourse_text'].values\n",
    "    label_list_ = df.loc[:,'discourse_type'].values\n",
    "    id_list = df.loc[:,'id'].values\n",
    "    type_dict = {'Lead':1,'Position':2,'Claim':3,'Counterclaim':4,'Rebuttal':5,'Evidence':6,'Concluding Statement':7}\n",
    "    label_list = []\n",
    "    for i in label_list_:\n",
    "        label_list.append(type_dict[i])\n",
    "    if len(data_list)!=len(label_list):\n",
    "        return 'length bug'\n",
    "    n = len(data_list)\n",
    "    for data_ in range(n):\n",
    "        sentence_split = data_list[data_].split(\".\")[:-1] if data_list[data_].split('.')[-1] == str('') else data_list[data_].split(\".\")\n",
    "        label_split = [label_list[data_] for i in range(len(sentence_split))]\n",
    "        id_split = [id_list[data_] for i in range(len(sentence_split))]\n",
    "        for j in range(len(sentence_split)):\n",
    "            if sentence_split[j]!=' ':\n",
    "                data.append(sentence_split[j].lower()) \n",
    "                labels.append(label_split[j])\n",
    "                ids.append(id_split[j])\n",
    "    data_article, labels_article = [],[]\n",
    "    data_sentence, labels_sentence = [],[]\n",
    "    for i in range(len(ids)-1):\n",
    "        if ids[i]==ids[i+1]: \n",
    "            data_sentence.append(data[i])\n",
    "            labels_sentence.append(labels[i])\n",
    "        else:\n",
    "            data_article.append(data_sentence)\n",
    "            labels_article.append(labels_sentence)\n",
    "            data_sentence,labels_sentence = [],[]\n",
    "    return data_article , labels_article\n",
    "\n",
    "\n",
    "train_data = data_loading('./train.csv')\n",
    "print('训练集数目：', len(train_data[0]))\n",
    "#print(train_data[0][0],train_data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mlt\n",
    "plt.hist([len(train_data[1][i]) for i in range(len(train_data[1]))],bins = 10,rwidth=1, range=(1,80))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 2, 6, 6, 6, 6, 6, 6, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 7], [2, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7], [1, 1, 1, 2, 3, 6, 6, 6, 4, 4, 5, 5, 7], [1, 1, 1, 2, 3, 3, 3, 3, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7], [1, 1, 1, 1, 2, 2, 3, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 7]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "article,labels_ = train_data[0],train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_art = []\n",
    "attention_masks_art = []\n",
    "labels = []\n",
    "# For every sentence...\n",
    "for art in range(len(article)):\n",
    "    input_ids_sent = []\n",
    "    attention_masks_sent = []\n",
    "    for sent in article[art]:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 32,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids_sent.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_sent.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "    input_ids_sent = torch.cat(input_ids_sent, dim=0)\n",
    "    attention_masks_sent = torch.cat(attention_masks_sent, dim=0)\n",
    "    labels_sent = torch.tensor(labels_[art])\n",
    "\n",
    "    input_ids_art.append(input_ids_sent)\n",
    "    attention_masks_art.append(attention_masks_sent)\n",
    "    labels.append(labels_sent)\n",
    "\n",
    "#labels = [label.tolist() for label in labels]\n",
    "\n",
    "#print('Original: ', article[0])\n",
    "#print('Token IDs:', input_ids_art[:2],labels[:2])\n",
    "print(attention_masks_art[:2])\n",
    "#print(labels[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class article_dataset(Dataset):\n",
    "    def __init__(self,input,target,masks):\n",
    "        super(article_dataset,self).__init__()\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.masks = masks\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input[idx],self.masks[idx],self.target[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1a81de22280>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = article_dataset(input_ids_art,labels,attention_masks_art)\n",
    "dataloder = DataLoader(dataset,batch_size=5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [16, 32] at entry 0 and [49, 32] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-e92bc7a1203f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m's'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [16, 32] at entry 0 and [49, 32] at entry 1"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
