{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlt\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数目： 15593\n"
     ]
    }
   ],
   "source": [
    "def data_loading(data_dir):\n",
    "    data, labels, ids= [], [], []\n",
    "    df = pd.read_csv(data_dir)\n",
    "    data_list = df.loc[:,'discourse_text'].values\n",
    "    label_list_ = df.loc[:,'discourse_type'].values\n",
    "    id_list = df.loc[:,'id'].values\n",
    "    type_dict = {'Lead':1,'Position':2,'Claim':3,'Counterclaim':4,'Rebuttal':5,'Evidence':6,'Concluding Statement':7}\n",
    "    label_list = []\n",
    "    for i in label_list_:\n",
    "        label_list.append(type_dict[i])\n",
    "    if len(data_list)!=len(label_list):\n",
    "        return 'length bug'\n",
    "    n = len(data_list)\n",
    "    for data_ in range(n):\n",
    "        sentence_split = data_list[data_].split(\".\")[:-1] if data_list[data_].split('.')[-1] == str('') else data_list[data_].split(\".\")\n",
    "        label_split = [label_list[data_] for i in range(len(sentence_split))]\n",
    "        id_split = [id_list[data_] for i in range(len(sentence_split))]\n",
    "        for j in range(len(sentence_split)):\n",
    "            if sentence_split[j]!=' ':\n",
    "                data.append(sentence_split[j].lower()) \n",
    "                labels.append(label_split[j])\n",
    "                ids.append(id_split[j])\n",
    "    data_article, labels_article = [],[]\n",
    "    data_sentence, labels_sentence = [],[]\n",
    "    for i in range(len(ids)-1):\n",
    "        if ids[i]==ids[i+1]: \n",
    "            data_sentence.append(data[i])\n",
    "            labels_sentence.append(labels[i])\n",
    "        else:\n",
    "            data_article.append(data_sentence)\n",
    "            labels_article.append(labels_sentence)\n",
    "            data_sentence,labels_sentence = [],[]\n",
    "    return data_article , labels_article\n",
    "\n",
    "\n",
    "train_data = data_loading('./train.csv')\n",
    "print('训练集数目：', len(train_data[0]))\n",
    "#print(train_data[0][0],train_data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(train_data[1][i]) for i in range(len(train_data[1]))],bins = 10,rwidth=1, range=(1,80))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "article,labels_ = train_data[0],train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [tensor([[  101,  2715,  4286,  2651,  2024,  2467,  2006,  2037,  3042,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2024,  2467,  2006,  2037,  3042,  2062,  2084,  1019,\n",
      "          2847,  1037,  2154,  2053,  2644,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2035,  2027,  2079,  2003,  3793,  2067,  1998,  2830,  1998,\n",
      "          2074,  2031,  2177, 11834,  2015,  2006,  2591,  2865,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2130,  2079,  2009,  2096,  4439,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2024,  2070,  2428,  2919,  8465,  2043,  4933,  6433,\n",
      "          2043,  2009,  3310,  2000,  1037,  3042,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2070,  3056,  2752,  1999,  1996,  2142,  2163,  7221, 11640,\n",
      "          2013,  2465,  4734,  2074,  2138,  1997,  2009,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2043,  2111,  2031, 11640,  1010,  2027,  2113,  2055,  3056,\n",
      "         18726,  2008,  2027,  2031,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 18726,  2066,  9130, 10474, 16021, 23091,  1998, 10245,  7507,\n",
      "          2102,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2061,  2066,  2065,  1037,  2767,  5829,  2185,  1998,  2017,\n",
      "          2215,  2000,  2022,  1999,  3967,  2017,  2064,  2145,  2022,  1999,\n",
      "          3967,  2011, 14739,  6876,  2030,  3793,  7696,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2111,  2467,  2031,  2367,  3971,  2129,  2000, 10639,  2007,\n",
      "          1037,  3042,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 11640,  2031,  2904,  2349,  2000,  2256,  4245,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  4439,  2003,  2028,  1997,  1996,  2126,  2129,  2000,  2131,\n",
      "          2105,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2111,  2467,  2022,  2006,  2037, 11640,  2096,  2725,  2009,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2029,  2064,  3426,  3809,  3471,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2008,  1005,  1055,  2339,  2045,  1005,  1055,  1037,  2518,\n",
      "          2008,  1005,  1055,  2170,  2053,  3793,  2075,  2096,  4439,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2008,  1005,  1055,  1037,  2428,  2590,  2518,  2000,  3342,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2070,  2111,  2145,  2079,  2009,  2138,  2027,  2228,  2009,\n",
      "          1005,  1055,  5236,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2053,  3043,  2054,  2027,  2079,  2027,  2145,  2031,  2000,\n",
      "         15470,  2009,  2138,  2008,  1005,  1055,  1996,  2069,  2126,  2129,\n",
      "          2106,  2002,  3828,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2823,  2006,  1996,  2739,  2045,  2003,  2593,  2019,  4926,\n",
      "          2030,  1037,  5920,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2009,  2453,  9125,  2619,  2025,  2559,  2073,  2027,  1005,\n",
      "          2128,  2183,  2030,  1056, 28394,  2102,  2008,  2619,  2741,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2009,  2593,  4544,  2030,  2331,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2065,  1037,  8075,  2193,  2758,  1045,  1005,  1049,  2183,\n",
      "          2000,  3102,  2017,  1998,  2027,  2113,  2073,  2017,  2444,  2021,\n",
      "          2017,  2123,  1005,  1056,  2113,  1996,  2711,  1005,  1055,  3967,\n",
      "          1010,   102],\n",
      "        [  101,  2029,  2064,  2203,  2039,  2428,  6649,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 11640,  2024,  2986,  2000,  2224,  1998,  2009,  1005,  1055,\n",
      "          2036,  1996,  2190,  2126,  2000,  2272,  2058,  2393,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2065,  2017,  2175,  2083,  1037,  3291,  1998,  2017,  2064,\n",
      "          1005,  1056,  2424,  2393,  2017,  1010,  2467,  2031,  1037,  3042,\n",
      "          2045,  2007,  2017,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2130,  2295, 11640,  2024,  2109,  2471,  2296,  2154,  2004,\n",
      "          2146,  2004,  2017,  1005,  2128,  3647,  2009,  2052,  2272,  2046,\n",
      "          2224,  2065,  2017,  2131,  2046,  4390,   102,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2191,  2469,  2017,  2079,  2025,  2022,  2066,  2023,  3042,\n",
      "          2096,  2017,  1005,  2128,  1999,  1996,  2690,  1997,  4439,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1996,  2739,  2467,  7172,  2043,  2111,  2079,  2242,  5236,\n",
      "          2105,  2008,  7336,  2037, 11640,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), tensor([[  101,  6853,  2323,  2025,  2022,  2583,  2000,  2224, 11640,  2096,\n",
      "          4082,  1037,  4316,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2040,  2109,  2037,  3042,  2096,  4082,  1037,  4316,\n",
      "          2024,  2087,  3497,  2000,  2131,  2046,  2019,  4926,  2008,  2071,\n",
      "          2022, 10611,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  2019,  3720,  2011,  1996,  9586, 17840,  3813,\n",
      "          1010,  2538,  1003,  1997, 13496,  2008,  2020,  2112,  1997,  1037,\n",
      "         10611,  2482,  4926,  2001,  2349,  2000, 11640,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  1996,  2168,  3720,  1010,  3486,  1003,  2113,\n",
      "          1996,  3891,  2021,  3613,  2478,  2037, 11640,  2096,  2006,  1996,\n",
      "          2346,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2023,  3065,  2008,  2049,  3458,  4795,  1998, 20868,  6072,\n",
      "         26029, 19307,  1997,  6853,  2025,  2000,  2022,  3929,  5204,  1997,\n",
      "          2037, 11301,  2096,  4439,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2323,  2022,  2583,  2000, 10152,  2302,  2151, 14836,\n",
      "          2015,  1010,  2138,  2009,  2071,  2022, 10611,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  2178,  3720,  1010,  1000, 11116,  4439,  1000,\n",
      "          2011,  1996, 18699, 27110,  1010,  2045,  2038,  2525,  2042,  2055,\n",
      "          1017,  1010,  2199,  3042,  3141,  2482,  4926,  6677,  2144,  2418,\n",
      "           102,     0],\n",
      "        [  101,  1996,  3720,  2163,  2008,  9458,  2131,  2205, 11116,  2007,\n",
      "          2037, 11640,  1010,  2029,  5320,  2037,  4926,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 13436,  2008,  2064,  2022,  4089,  9511,  2011,  7995,  2006,\n",
      "          1996,  2346,  1998,  2025,  1037,  3042,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2323,  2025,  2022,  2583,  2000,  2224,  2037, 11640,\n",
      "          2012,  2035,  2096,  4439,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1999,  7091,  1010,  6853,  2323,  2025,  2583,  2000,  2147,\n",
      "          1037,  4316,  2096,  2478,  2037,  3526,  3042,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2040,  3594,  2037, 11640,  2096,  4082,  1037,  4316,\n",
      "          1998,  2024,  3497,  2000,  2031,  2019,  4926,  2059,  2216,  2040,\n",
      "          2123,  1005,  1056,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])] [tensor([1, 1, 1, 1, 2, 6, 6, 6, 6, 6, 6, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3,\n",
      "        6, 6, 6, 7]), tensor([2, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7])]\n",
      "[tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])]\n",
      "[tensor([1, 1, 1, 2, 3, 6, 6, 6, 4, 4, 5, 5, 7]), tensor([1, 1, 1, 2, 3, 3, 3, 3, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7])]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_art = []\n",
    "attention_masks_art = []\n",
    "labels = []\n",
    "# For every sentence...\n",
    "for art in range(len(article)):\n",
    "    input_ids_sent = []\n",
    "    attention_masks_sent = []\n",
    "    for sent in article[art]:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 32,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids_sent.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_sent.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "\n",
    "    input_ids_sent = torch.cat(input_ids_sent, dim=0)\n",
    "    attention_masks_sent = torch.cat(attention_masks_sent, dim=0)\n",
    "    labels_sent = torch.tensor(labels_[art])\n",
    "\n",
    "    input_ids_art.append(input_ids_sent)\n",
    "    attention_masks_art.append(attention_masks_sent)\n",
    "    labels.append(labels_sent)\n",
    "\n",
    "#labels = [label.tolist() for label in labels]\n",
    "\n",
    "#print('Original: ', article[0])\n",
    "print('Token IDs:', input_ids_art[:2],labels[:2])\n",
    "print(attention_masks_art[:2])\n",
    "print(labels[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对文章层面进行padding，并记录文章有效句子个数\n",
    "def padding(id,mask,label,max_sent_num=50,max_sent_len=32):     #确保输入的lst为list列表结构，其中每一个元素为pytorchtensor。max_sent_len需要与前面tokenizer里参数对应上\n",
    "    valid_num = []\n",
    "    for i in range(len(id)):\n",
    "        id[i] = id[i].tolist()\n",
    "        mask[i] = mask[i].tolist()\n",
    "        label[i] = label[i].tolist()\n",
    "        valid_num.append(len(id[i]) if len(id[i])<=50 else 50)\n",
    "        if len(id[i])<max_sent_num:\n",
    "            id[i] = id[i]+[[101,102]+[0 for _ in range(max_sent_len-2)] for __ in range(max_sent_num-len(id[i]))]\n",
    "            mask[i] = mask[i]+[[1,1]+[0 for _ in range(max_sent_len-2)] for __ in range(max_sent_num-len(mask[i]))]\n",
    "            label[i] = label[i] + [0 for _ in range(max_sent_num-len(label[i]))]\n",
    "        if len(id[i])>max_sent_num:\n",
    "            id[i] = id[i][:20]+id[i][-30:]\n",
    "            mask[i] = mask[i][:20]+id[i][-30:]\n",
    "            label[i] = label[i][:20]+label[i][-30:]\n",
    "    #return id,mask,valid_num\n",
    "    return torch.tensor(id),torch.tensor(mask),torch.tensor(label),valid_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_art,attention_masks_art,labels,valid_num = padding(input_ids_art,attention_masks_art,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15593, 50, 32]),\n",
       " torch.Size([15593, 50, 32]),\n",
       " torch.Size([15593, 50]),\n",
       " 15593)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_art.size(),attention_masks_art.size(),labels.size(),len(valid_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class article_dataset(Dataset):\n",
    "    def __init__(self,input,masks,labels,valid_num):\n",
    "        super(article_dataset,self).__init__()\n",
    "        self.input = input\n",
    "        self.labels = labels\n",
    "        self.masks = masks\n",
    "        self.valid_num = valid_num\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input[idx],self.masks[idx],self.labels[idx],self.valid_num[idx]      #input和masks维度为sentence_num,sentence_len，label维度 sentence_num\n",
    "    def __len__(self):                                                                 #valid_num为int\n",
    "        return len(self.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章中句子个数不一致问题暂定解决办法：\n",
    "    1：找到合适的sentence_num（尽量所有文章的长度均小于该值）。\n",
    "    2：对小于该长度的文章，在处理完tokenizer之前进行空句子补齐，并记录其有效长度。（其会在tokennizer阶段被标注为[101,102,...,0]）。\n",
    "    3：所有用于补齐的空句子不设置类别，在处理空句子时记录每篇文章的有效句子长度。句子有效长度在每一个batch中使用一个list储存（想办法在dataset中实现）\n",
    "    4：在模型框架中，仅输出有效句子的损失并进行后向传播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = article_dataset(input_ids_art,attention_masks_art,labels,valid_num)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=2,drop_last=True)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset,sampler=val_sampler,batch_size=2,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 101, 2256, 4054,  ...,    0,    0,    0],\n",
      "         [ 101, 2002, 1005,  ...,    0,    0,    0],\n",
      "         [ 101, 1045, 2514,  ...,    0,    0,    0],\n",
      "         ...,\n",
      "         [ 101,  102,    0,  ...,    0,    0,    0],\n",
      "         [ 101,  102,    0,  ...,    0,    0,    0],\n",
      "         [ 101,  102,    0,  ...,    0,    0,    0]],\n",
      "\n",
      "        [[ 101, 2429, 2000,  ..., 5533, 2011,  102],\n",
      "         [ 101, 3350, 2013,  ...,    0,    0,    0],\n",
      "         [ 101, 2026, 5448,  ..., 4390, 2084,  102],\n",
      "         ...,\n",
      "         [ 101,  102,    0,  ...,    0,    0,    0],\n",
      "         [ 101,  102,    0,  ...,    0,    0,    0],\n",
      "         [ 101,  102,    0,  ...,    0,    0,    0]]]) tensor([[[1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         ...,\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 0, 0, 0],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0],\n",
      "         [1, 1, 0,  ..., 0, 0, 0]]]) tensor([[1, 2, 3, 6, 3, 6, 6, 6, 4, 5, 6, 6, 3, 6, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 2, 2, 3, 6, 6, 3, 6, 6, 6, 3, 6, 3, 6, 6, 6, 7, 7, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]) tensor([16, 19])\n",
      "len(train_dataloader)= 7016\n"
     ]
    }
   ],
   "source": [
    "for i, b in enumerate(train_dataloader):\n",
    "    print (b[0],b[1],b[2],b[3])\n",
    "    break\n",
    "print('len(train_dataloader)=', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self,bert_name,hidden_size_rnn,num_layers_rnn,classes,dropout):     #valid_num表示该input中有效的sentence个数\n",
    "        super(HAN,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_name)\n",
    "        hidden_size_bert = self.bert.config.hidden_size\n",
    "        self.rnn = nn.LSTM(input_size=hidden_size_bert, hidden_size=hidden_size_rnn, num_layers=num_layers_rnn, bias=True, dropout=dropout,batch_first=True)\n",
    "        self.outlayer = nn.Linear(hidden_size_rnn,classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.classes = classes\n",
    "        \n",
    "    def forward(self,input_ids,input_masks,valid_num,labels=None):  #input维度 batchsize,num_sentence,sent_len , label维度batchsize,num_sentence,sent_len\n",
    "        batch_size,sentence_num,sentence_len = input_ids.size(0),input_ids.size(1),input_ids.size(2)\n",
    "        hidden_size_bert = self.bert.config.hidden_size\n",
    "        input_ids,input_masks = input_ids.view(-1,sentence_len),input_masks.view(-1,sentence_len)\n",
    "        out = self.bert(input_ids,input_masks)      #out维度 batchsize*num_sentence,sen_len,hidden_size\n",
    "        out = out[0][:,0,:]     #out维度 batchsize*num_sentence,hidden_size\n",
    "        out = out.view(batch_size,sentence_num,hidden_size_bert)    #out维度转化为batchsize,num_sentence,hidden_size\n",
    "        self.rnn.flatten_parameters()\n",
    "        outputs,_ = self.rnn(out)   #outputs维度 batchsize,num_sentence,hidden_size\n",
    "        outputs = self.outlayer(outputs)    #outputs维度为 batchsize,num_sentence,classes\n",
    "        output = []\n",
    "        for batch in range(len(outputs)):\n",
    "            output.append(outputs[batch][0:valid_num[batch],:].tolist())\n",
    "        if labels is not None:\n",
    "            outputs_loss,labels = outputs.view(-1,self.classes),labels.view(-1)\n",
    "            #outputs = torch.softmax(outputs)\n",
    "            loss = self.loss(outputs_loss,labels)\n",
    "            return loss,outputs\n",
    "        else:\n",
    "            return 0,outputs\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HAN(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (rnn): LSTM(768, 256, batch_first=True)\n",
       "  (outlayer): Linear(in_features=256, out_features=8, bias=True)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HAN('bert-base-uncased',256,1,8,0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "epochs = 2\n",
    "# training steps 的数量: [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# 设计 learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_acc(preds, labels,classes):     \n",
    "    preds = preds.view(-1,classes)\n",
    "    labels = labels.view(-1)\n",
    "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float()      #eq里面的两个参数的shape=torch.Size([16])    \n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):    \n",
    "    elapsed_rounded = int(round((elapsed)))    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def train(model,optimizer):\n",
    "    t0 = time.time()\n",
    "    avg_loss, avg_acc = [],[]   \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # 每隔40个batch 输出一下所用时间.\n",
    "        if step % 40 == 0 and not step == 0:            \n",
    "            elapsed = format_time(time.time() - t0)             \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            print(np.array(avg_loss).mean())\n",
    "        input_id,input_mask,label,valid_num_tensor = batch[0].long().to(device),batch[1].long().to(device),batch[2].long().to(device),batch[3]\n",
    "        valid_num = valid_num_tensor.tolist()\n",
    "        output = model(input_id,input_mask,valid_num,label)\n",
    "        loss,outputs = output[0],output[1]\n",
    "        avg_loss.append(loss.item())\n",
    "       \n",
    "        acc = binary_acc(outputs, label,classes=8)\n",
    "        avg_acc.append(acc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 1.0)      #大于1的梯度将其设为1.0, 以防梯度爆炸\n",
    "        optimizer.step()              #更新模型参数\n",
    "        scheduler.step()              #更新learning rate\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    return avg_loss, avg_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  7,016.    Elapsed: 0:00:10.\n",
      "1.1060986936092376\n",
      "  Batch    80  of  7,016.    Elapsed: 0:00:19.\n",
      "0.8712756294757128\n",
      "  Batch   120  of  7,016.    Elapsed: 0:00:28.\n",
      "0.7888670365015665\n",
      "  Batch   160  of  7,016.    Elapsed: 0:00:37.\n",
      "0.7382551541551947\n",
      "  Batch   200  of  7,016.    Elapsed: 0:00:46.\n",
      "0.6925283728539944\n",
      "  Batch   240  of  7,016.    Elapsed: 0:00:55.\n",
      "0.6769735785822074\n",
      "  Batch   280  of  7,016.    Elapsed: 0:01:04.\n",
      "0.6572467629398618\n",
      "  Batch   320  of  7,016.    Elapsed: 0:01:13.\n",
      "0.6359179001301527\n",
      "  Batch   360  of  7,016.    Elapsed: 0:01:22.\n",
      "0.6201376038293044\n",
      "  Batch   400  of  7,016.    Elapsed: 0:01:31.\n",
      "0.6134412462636828\n",
      "  Batch   440  of  7,016.    Elapsed: 0:01:40.\n",
      "0.6011039934036406\n",
      "  Batch   480  of  7,016.    Elapsed: 0:01:49.\n",
      "0.5883931877091527\n",
      "  Batch   520  of  7,016.    Elapsed: 0:01:58.\n",
      "0.5789033500334391\n",
      "  Batch   560  of  7,016.    Elapsed: 0:02:07.\n",
      "0.569499792318259\n",
      "  Batch   600  of  7,016.    Elapsed: 0:02:17.\n",
      "0.5607113386938969\n",
      "  Batch   640  of  7,016.    Elapsed: 0:02:26.\n",
      "0.555895803309977\n",
      "  Batch   680  of  7,016.    Elapsed: 0:02:35.\n",
      "0.5491440801059498\n",
      "  Batch   720  of  7,016.    Elapsed: 0:02:44.\n",
      "0.5430956378165218\n",
      "  Batch   760  of  7,016.    Elapsed: 0:02:53.\n",
      "0.5359923555074554\n",
      "  Batch   800  of  7,016.    Elapsed: 0:03:03.\n",
      "0.5288721494935453\n",
      "  Batch   840  of  7,016.    Elapsed: 0:03:12.\n",
      "0.5223728017083236\n",
      "  Batch   880  of  7,016.    Elapsed: 0:03:21.\n",
      "0.5166079135442322\n",
      "  Batch   920  of  7,016.    Elapsed: 0:03:30.\n",
      "0.5115647437941769\n",
      "  Batch   960  of  7,016.    Elapsed: 0:03:39.\n",
      "0.5078528145172944\n",
      "  Batch 1,000  of  7,016.    Elapsed: 0:03:49.\n",
      "0.5015593202710151\n",
      "  Batch 1,040  of  7,016.    Elapsed: 0:03:58.\n",
      "0.4973533621057868\n",
      "  Batch 1,080  of  7,016.    Elapsed: 0:04:07.\n",
      "0.4931392383934171\n",
      "  Batch 1,120  of  7,016.    Elapsed: 0:04:16.\n",
      "0.4888082413934171\n",
      "  Batch 1,160  of  7,016.    Elapsed: 0:04:25.\n",
      "0.48468983736017657\n",
      "  Batch 1,200  of  7,016.    Elapsed: 0:04:34.\n",
      "0.48254874523729085\n",
      "  Batch 1,240  of  7,016.    Elapsed: 0:04:44.\n",
      "0.4802761867882744\n",
      "  Batch 1,280  of  7,016.    Elapsed: 0:04:53.\n",
      "0.4777330013923347\n",
      "  Batch 1,320  of  7,016.    Elapsed: 0:05:02.\n",
      "0.47430926301714144\n",
      "  Batch 1,360  of  7,016.    Elapsed: 0:05:11.\n",
      "0.4709701045361512\n",
      "  Batch 1,400  of  7,016.    Elapsed: 0:05:19.\n",
      "0.46821350522339344\n",
      "  Batch 1,440  of  7,016.    Elapsed: 0:05:28.\n",
      "0.4651088424130446\n",
      "  Batch 1,480  of  7,016.    Elapsed: 0:05:37.\n",
      "0.4625769284327288\n",
      "  Batch 1,520  of  7,016.    Elapsed: 0:05:45.\n",
      "0.45993120288476347\n",
      "  Batch 1,560  of  7,016.    Elapsed: 0:05:53.\n",
      "0.4574253351212694\n",
      "  Batch 1,600  of  7,016.    Elapsed: 0:06:01.\n",
      "0.45477725808043035\n",
      "  Batch 1,640  of  7,016.    Elapsed: 0:06:09.\n",
      "0.45284740794450046\n",
      "  Batch 1,680  of  7,016.    Elapsed: 0:06:18.\n",
      "0.4500825874995263\n",
      "  Batch 1,720  of  7,016.    Elapsed: 0:06:27.\n",
      "0.44862546225533234\n",
      "  Batch 1,760  of  7,016.    Elapsed: 0:06:37.\n",
      "0.44533371949483724\n",
      "  Batch 1,800  of  7,016.    Elapsed: 0:06:46.\n",
      "0.44539860193514164\n",
      "  Batch 1,840  of  7,016.    Elapsed: 0:06:56.\n",
      "0.44335300729245597\n",
      "  Batch 1,880  of  7,016.    Elapsed: 0:07:05.\n",
      "0.4419108014910462\n",
      "  Batch 1,920  of  7,016.    Elapsed: 0:07:15.\n",
      "0.44027641995344313\n",
      "  Batch 1,960  of  7,016.    Elapsed: 0:07:24.\n",
      "0.43799717965326745\n",
      "  Batch 2,000  of  7,016.    Elapsed: 0:07:34.\n",
      "0.4356773367896676\n",
      "  Batch 2,040  of  7,016.    Elapsed: 0:07:43.\n",
      "0.43380327114433637\n",
      "  Batch 2,080  of  7,016.    Elapsed: 0:07:52.\n",
      "0.4324487006255927\n",
      "  Batch 2,120  of  7,016.    Elapsed: 0:08:00.\n",
      "0.4309853145290377\n",
      "  Batch 2,160  of  7,016.    Elapsed: 0:08:08.\n",
      "0.42902187705384914\n",
      "  Batch 2,200  of  7,016.    Elapsed: 0:08:16.\n",
      "0.42717625590549274\n",
      "  Batch 2,240  of  7,016.    Elapsed: 0:08:24.\n",
      "0.4256372143481193\n",
      "  Batch 2,280  of  7,016.    Elapsed: 0:08:32.\n",
      "0.4245902979942529\n",
      "  Batch 2,320  of  7,016.    Elapsed: 0:08:41.\n",
      "0.4224074125900094\n",
      "  Batch 2,360  of  7,016.    Elapsed: 0:08:51.\n",
      "0.42126960803959834\n",
      "  Batch 2,400  of  7,016.    Elapsed: 0:09:00.\n",
      "0.4201380432986965\n",
      "  Batch 2,440  of  7,016.    Elapsed: 0:09:10.\n",
      "0.41851451135561113\n",
      "  Batch 2,480  of  7,016.    Elapsed: 0:09:19.\n",
      "0.41768220171272274\n",
      "  Batch 2,520  of  7,016.    Elapsed: 0:09:29.\n",
      "0.416080500886199\n",
      "  Batch 2,560  of  7,016.    Elapsed: 0:09:38.\n",
      "0.4143641801114427\n",
      "  Batch 2,600  of  7,016.    Elapsed: 0:09:47.\n",
      "0.4129886044103366\n",
      "  Batch 2,640  of  7,016.    Elapsed: 0:09:57.\n",
      "0.41145314431721064\n",
      "  Batch 2,680  of  7,016.    Elapsed: 0:10:06.\n",
      "0.40975573374542285\n",
      "  Batch 2,720  of  7,016.    Elapsed: 0:10:16.\n",
      "0.40811697233413513\n",
      "  Batch 2,760  of  7,016.    Elapsed: 0:10:25.\n",
      "0.4074610326250178\n",
      "  Batch 2,800  of  7,016.    Elapsed: 0:10:35.\n",
      "0.40688549079267045\n",
      "  Batch 2,840  of  7,016.    Elapsed: 0:10:44.\n",
      "0.40575605879839455\n",
      "  Batch 2,880  of  7,016.    Elapsed: 0:10:54.\n",
      "0.4051901994454157\n",
      "  Batch 2,920  of  7,016.    Elapsed: 0:11:03.\n",
      "0.40386250706976407\n",
      "  Batch 2,960  of  7,016.    Elapsed: 0:11:13.\n",
      "0.40347174813819897\n",
      "  Batch 3,000  of  7,016.    Elapsed: 0:11:22.\n",
      "0.4025533689185977\n",
      "  Batch 3,040  of  7,016.    Elapsed: 0:11:31.\n",
      "0.4010359427499536\n",
      "  Batch 3,080  of  7,016.    Elapsed: 0:11:41.\n",
      "0.40022597358646717\n",
      "  Batch 3,120  of  7,016.    Elapsed: 0:11:50.\n",
      "0.3992164632353263\n",
      "  Batch 3,160  of  7,016.    Elapsed: 0:12:00.\n",
      "0.3979291886232699\n",
      "  Batch 3,200  of  7,016.    Elapsed: 0:12:09.\n",
      "0.3969528815103695\n",
      "  Batch 3,240  of  7,016.    Elapsed: 0:12:19.\n",
      "0.3961611762550878\n",
      "  Batch 3,280  of  7,016.    Elapsed: 0:12:28.\n",
      "0.3952994336919268\n",
      "  Batch 3,320  of  7,016.    Elapsed: 0:12:38.\n",
      "0.3941212945853371\n",
      "  Batch 3,360  of  7,016.    Elapsed: 0:12:47.\n",
      "0.39357117094276917\n",
      "  Batch 3,400  of  7,016.    Elapsed: 0:12:57.\n",
      "0.39275808788178596\n",
      "  Batch 3,440  of  7,016.    Elapsed: 0:13:06.\n",
      "0.3920710463791566\n",
      "  Batch 3,480  of  7,016.    Elapsed: 0:13:16.\n",
      "0.39118141514602406\n",
      "  Batch 3,520  of  7,016.    Elapsed: 0:13:25.\n",
      "0.39026036590380087\n",
      "  Batch 3,560  of  7,016.    Elapsed: 0:13:35.\n",
      "0.38998433660213533\n",
      "  Batch 3,600  of  7,016.    Elapsed: 0:13:44.\n",
      "0.3888873462027146\n",
      "  Batch 3,640  of  7,016.    Elapsed: 0:13:53.\n",
      "0.3880224119462482\n",
      "  Batch 3,680  of  7,016.    Elapsed: 0:14:03.\n",
      "0.38755050610912883\n",
      "  Batch 3,720  of  7,016.    Elapsed: 0:14:12.\n",
      "0.3864944502030329\n",
      "  Batch 3,760  of  7,016.    Elapsed: 0:14:22.\n",
      "0.38579401356623844\n",
      "  Batch 3,800  of  7,016.    Elapsed: 0:14:31.\n",
      "0.38498945186600875\n",
      "  Batch 3,840  of  7,016.    Elapsed: 0:14:41.\n",
      "0.3843310203616663\n",
      "  Batch 3,880  of  7,016.    Elapsed: 0:14:50.\n",
      "0.38378275739432305\n",
      "  Batch 3,920  of  7,016.    Elapsed: 0:15:00.\n",
      "0.38288387357428366\n",
      "  Batch 3,960  of  7,016.    Elapsed: 0:15:09.\n",
      "0.3822888345730425\n",
      "  Batch 4,000  of  7,016.    Elapsed: 0:15:18.\n",
      "0.38142200968414547\n",
      "  Batch 4,040  of  7,016.    Elapsed: 0:15:26.\n",
      "0.38101784376298437\n",
      "  Batch 4,080  of  7,016.    Elapsed: 0:15:34.\n",
      "0.38017357022446746\n",
      "  Batch 4,120  of  7,016.    Elapsed: 0:15:42.\n",
      "0.37968556773351525\n",
      "  Batch 4,160  of  7,016.    Elapsed: 0:15:50.\n",
      "0.37899753258814317\n",
      "  Batch 4,200  of  7,016.    Elapsed: 0:15:58.\n",
      "0.3783867398951025\n",
      "  Batch 4,240  of  7,016.    Elapsed: 0:16:08.\n",
      "0.3780358938500285\n",
      "  Batch 4,280  of  7,016.    Elapsed: 0:16:16.\n",
      "0.3775573646458231\n",
      "  Batch 4,320  of  7,016.    Elapsed: 0:16:24.\n",
      "0.37711020499743797\n",
      "  Batch 4,360  of  7,016.    Elapsed: 0:16:33.\n",
      "0.3762657151292634\n",
      "  Batch 4,400  of  7,016.    Elapsed: 0:16:41.\n",
      "0.3756564430130476\n",
      "  Batch 4,440  of  7,016.    Elapsed: 0:16:49.\n",
      "0.37501598399904396\n",
      "  Batch 4,480  of  7,016.    Elapsed: 0:16:57.\n",
      "0.3744562750948327\n",
      "  Batch 4,520  of  7,016.    Elapsed: 0:17:05.\n",
      "0.3739652033582066\n",
      "  Batch 4,560  of  7,016.    Elapsed: 0:17:13.\n",
      "0.37338967454505334\n",
      "  Batch 4,600  of  7,016.    Elapsed: 0:17:21.\n",
      "0.3729302452877164\n",
      "  Batch 4,640  of  7,016.    Elapsed: 0:17:29.\n",
      "0.37229211323509187\n",
      "  Batch 4,680  of  7,016.    Elapsed: 0:17:37.\n",
      "0.3717227057211547\n",
      "  Batch 4,720  of  7,016.    Elapsed: 0:17:45.\n",
      "0.3711660651569018\n",
      "  Batch 4,760  of  7,016.    Elapsed: 0:17:53.\n",
      "0.37041538768925336\n",
      "  Batch 4,800  of  7,016.    Elapsed: 0:18:00.\n",
      "0.3698038309269274\n",
      "  Batch 4,840  of  7,016.    Elapsed: 0:18:09.\n",
      "0.36919730975454257\n",
      "  Batch 4,880  of  7,016.    Elapsed: 0:18:16.\n",
      "0.36888232309157487\n",
      "  Batch 4,920  of  7,016.    Elapsed: 0:18:25.\n",
      "0.36809418294427354\n",
      "  Batch 4,960  of  7,016.    Elapsed: 0:18:33.\n",
      "0.36742105743276976\n",
      "  Batch 5,000  of  7,016.    Elapsed: 0:18:41.\n",
      "0.366898451384902\n",
      "  Batch 5,040  of  7,016.    Elapsed: 0:18:49.\n",
      "0.36624198736593366\n",
      "  Batch 5,080  of  7,016.    Elapsed: 0:18:57.\n",
      "0.36585584399913706\n",
      "  Batch 5,120  of  7,016.    Elapsed: 0:19:05.\n",
      "0.3649367102829274\n",
      "  Batch 5,160  of  7,016.    Elapsed: 0:19:13.\n",
      "0.36409830159341644\n",
      "  Batch 5,200  of  7,016.    Elapsed: 0:19:21.\n",
      "0.3637999304426977\n",
      "  Batch 5,240  of  7,016.    Elapsed: 0:19:29.\n",
      "0.3632917951038889\n",
      "  Batch 5,280  of  7,016.    Elapsed: 0:19:37.\n",
      "0.36269365719265556\n",
      "  Batch 5,320  of  7,016.    Elapsed: 0:19:44.\n",
      "0.3623360670878923\n",
      "  Batch 5,360  of  7,016.    Elapsed: 0:19:52.\n",
      "0.3619012061645513\n",
      "  Batch 5,400  of  7,016.    Elapsed: 0:20:00.\n",
      "0.36128594299709355\n",
      "  Batch 5,440  of  7,016.    Elapsed: 0:20:08.\n",
      "0.3610994474195382\n",
      "  Batch 5,480  of  7,016.    Elapsed: 0:20:16.\n",
      "0.36052678685608136\n",
      "  Batch 5,520  of  7,016.    Elapsed: 0:20:24.\n",
      "0.3600111490034539\n",
      "  Batch 5,560  of  7,016.    Elapsed: 0:20:32.\n",
      "0.3594526962383831\n",
      "  Batch 5,600  of  7,016.    Elapsed: 0:20:40.\n",
      "0.35908523994631003\n",
      "  Batch 5,640  of  7,016.    Elapsed: 0:20:48.\n",
      "0.3584889710610006\n",
      "  Batch 5,680  of  7,016.    Elapsed: 0:20:56.\n",
      "0.35795002798615416\n",
      "  Batch 5,720  of  7,016.    Elapsed: 0:21:04.\n",
      "0.3574358189139854\n",
      "  Batch 5,760  of  7,016.    Elapsed: 0:21:11.\n",
      "0.3569227826561675\n",
      "  Batch 5,800  of  7,016.    Elapsed: 0:21:19.\n",
      "0.3564773544771918\n",
      "  Batch 5,840  of  7,016.    Elapsed: 0:21:27.\n",
      "0.3562624292894688\n",
      "  Batch 5,880  of  7,016.    Elapsed: 0:21:36.\n",
      "0.3560850304621113\n",
      "  Batch 5,920  of  7,016.    Elapsed: 0:21:44.\n",
      "0.3558852406936018\n",
      "  Batch 5,960  of  7,016.    Elapsed: 0:21:52.\n",
      "0.3554687256293509\n",
      "  Batch 6,000  of  7,016.    Elapsed: 0:22:00.\n",
      "0.35496855786939463\n",
      "  Batch 6,040  of  7,016.    Elapsed: 0:22:08.\n",
      "0.354751168625639\n",
      "  Batch 6,080  of  7,016.    Elapsed: 0:22:15.\n",
      "0.35437765320713976\n",
      "  Batch 6,120  of  7,016.    Elapsed: 0:22:23.\n",
      "0.35420106896290593\n",
      "  Batch 6,160  of  7,016.    Elapsed: 0:22:31.\n",
      "0.35377403172216826\n",
      "  Batch 6,200  of  7,016.    Elapsed: 0:22:39.\n",
      "0.3531393643769045\n",
      "  Batch 6,240  of  7,016.    Elapsed: 0:22:47.\n",
      "0.3528225882910192\n",
      "  Batch 6,280  of  7,016.    Elapsed: 0:22:55.\n",
      "0.3524746928269127\n",
      "  Batch 6,320  of  7,016.    Elapsed: 0:23:03.\n",
      "0.3521420568711113\n",
      "  Batch 6,360  of  7,016.    Elapsed: 0:23:11.\n",
      "0.35159862349805593\n",
      "  Batch 6,400  of  7,016.    Elapsed: 0:23:19.\n",
      "0.3515683684125543\n",
      "  Batch 6,440  of  7,016.    Elapsed: 0:23:27.\n",
      "0.35109451418649984\n",
      "  Batch 6,480  of  7,016.    Elapsed: 0:23:35.\n",
      "0.3508092454490884\n",
      "  Batch 6,520  of  7,016.    Elapsed: 0:23:42.\n",
      "0.3504752799531479\n",
      "  Batch 6,560  of  7,016.    Elapsed: 0:23:50.\n",
      "0.3501031474795238\n",
      "  Batch 6,600  of  7,016.    Elapsed: 0:23:58.\n",
      "0.34997091985166523\n",
      "  Batch 6,640  of  7,016.    Elapsed: 0:24:06.\n",
      "0.3496712592480906\n",
      "  Batch 6,680  of  7,016.    Elapsed: 0:24:15.\n",
      "0.34955354473626093\n",
      "  Batch 6,720  of  7,016.    Elapsed: 0:24:25.\n",
      "0.34915044156646\n",
      "  Batch 6,760  of  7,016.    Elapsed: 0:24:34.\n",
      "0.34875377511341105\n",
      "  Batch 6,800  of  7,016.    Elapsed: 0:24:44.\n",
      "0.34856869245660216\n",
      "  Batch 6,840  of  7,016.    Elapsed: 0:24:53.\n",
      "0.34824002072846855\n",
      "  Batch 6,880  of  7,016.    Elapsed: 0:25:03.\n",
      "0.3478489196296175\n",
      "  Batch 6,920  of  7,016.    Elapsed: 0:25:12.\n",
      "0.3474885168200025\n",
      "  Batch 6,960  of  7,016.    Elapsed: 0:25:22.\n",
      "0.34713178879485046\n",
      "  Batch 7,000  of  7,016.    Elapsed: 0:25:31.\n",
      "0.34675966433222805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3465887441930649, 0.881150228050171)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):    \n",
    "    avg_acc = []    \n",
    "    model.eval()         #表示进入测试模式\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_id,input_mask,label,valid_num_tensor = batch[0].long().to(device),batch[1].long().to(device),batch[2].long().to(device),batch[3]\n",
    "            valid_num = valid_num_tensor.tolist()\n",
    "            output = model(input_id,input_mask,valid_num)\n",
    "            acc = binary_acc(output[1], label,8)\n",
    "            avg_acc.append(acc)\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1,测试准确率=0.8987179487179487\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate(model)\n",
    "print(\"epoch={},测试准确率={}\".format(1, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = torch.tensor([[[  101,  2715,  4286,  2651,  2024,  2467,  2006,  2037,  3042,   102,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2027,  2024,  2467,  2006,  2037,  3042,  2062,  2084,  1019,\n",
    "          2847,  1037,  2154,  2053,  2644,   102,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]],\n",
    "     [[  101,  2035,  2027,  2079,  2003,  3793,  2067,  1998,  2830,  1998,\n",
    "          2074,  2031,  2177, 11834,  2015,  2006,  2591,  2865,   102,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2027,  2130,  2079,  2009,  2096,  4439,   102,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2027,  2024,  2070,  2428,  2919,  8465,  2043,  4933,  6433,\n",
    "          2043,  2009,  3310,  2000,  1037,  3042,   102,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "          [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]],\n",
    "     [[  101,  2070,  3056,  2752,  1999,  1996,  2142,  2163,  7221, 11640,\n",
    "          2013,  2465,  4734,  2074,  2138,  1997,  2009,   102,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2043,  2111,  2031, 11640,  1010,  2027,  2113,  2055,  3056,\n",
    "         18726,  2008,  2027,  2031,   102,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101, 18726,  2066,  9130, 10474, 16021, 23091,  1998, 10245,  7507,\n",
    "          2102,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2061,  2066,  2065,  1037,  2767,  5829,  2185,  1998,  2017,\n",
    "          2215,  2000,  2022,  1999,  3967,  2017,  2064,  2145,  2022,  1999,\n",
    "          3967,  2011, 14739,  6876,  2030,  3793,  7696,   102,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2111,  2467,  2031,  2367,  3971,  2129,  2000, 10639,  2007,\n",
    "          1037,  3042,   102,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.0651, device='cuda:0', grad_fn=<NllLossBackward0>),\n",
       " tensor([[[-1.7966,  4.3980,  0.4020,  0.1901, -0.9360, -0.9353,  0.1545,\n",
       "           -1.0947],\n",
       "          [-2.5727,  5.1894,  0.8326,  0.3240, -1.3824, -1.1878,  1.2407,\n",
       "           -2.4889],\n",
       "          [ 0.1295,  3.1832,  1.7087,  0.6569, -1.6410, -1.7807,  0.9052,\n",
       "           -3.3001],\n",
       "          [ 1.3653,  2.1004,  1.5693,  0.8682, -1.5904, -2.0274,  1.0314,\n",
       "           -3.6030],\n",
       "          [ 2.0191,  1.2507,  1.2019,  1.0116, -1.4569, -2.1221,  1.2355,\n",
       "           -3.6005],\n",
       "          [ 2.4864,  0.6088,  0.9000,  0.9784, -1.3357, -2.0788,  1.4124,\n",
       "           -3.6060]],\n",
       " \n",
       "         [[-1.9362,  3.3207,  0.1454,  0.0184, -0.9195, -0.3972,  1.0171,\n",
       "           -1.4163],\n",
       "          [-3.1478,  4.2940,  0.6983,  1.0907, -1.1747, -1.0116,  0.9189,\n",
       "           -2.4490],\n",
       "          [-3.6493,  4.3870,  1.7175,  1.6412, -1.2912, -1.4305,  0.7493,\n",
       "           -2.5792],\n",
       "          [-0.4283,  2.6628,  2.1850,  1.4274, -1.5540, -1.9612,  0.7348,\n",
       "           -3.5105],\n",
       "          [ 0.8044,  1.6718,  1.8696,  1.5861, -1.5071, -2.2025,  1.0268,\n",
       "           -3.7126],\n",
       "          [ 1.4947,  0.7857,  1.3643,  1.6002, -1.4124, -2.2846,  1.3391,\n",
       "           -3.7615]],\n",
       " \n",
       "         [[-2.2433,  3.4924,  0.0985,  0.4633, -0.2014, -0.4670,  0.7295,\n",
       "           -1.5447],\n",
       "          [-3.3759,  3.6013,  0.5075,  1.5759, -0.7180, -1.0380,  1.0924,\n",
       "           -2.2850],\n",
       "          [-3.4284,  3.8130,  0.5339,  0.8894, -1.0191, -1.0782,  2.4655,\n",
       "           -2.6956],\n",
       "          [-3.6183,  2.7284,  0.3677,  1.3831, -0.9447, -1.3096,  2.8830,\n",
       "           -2.5658],\n",
       "          [-3.7738,  2.8191,  0.4544,  2.1377, -1.0812, -1.9638,  2.3053,\n",
       "           -2.0540],\n",
       "          [ 0.1062,  1.3922,  1.1332,  1.4304, -1.4142, -2.3605,  1.6432,\n",
       "           -3.3509]]], device='cuda:0', grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "q = sentence.long().to(device)\n",
    "w = mask.long().to(device)\n",
    "r = torch.tensor([2,3,5])\n",
    "e = torch.tensor([[2,3,0,0,0,0],[5,3,2,0,0,0],[5,5,5,2,6,0]]).long().to(device)\n",
    "output = model(q,w,r,e)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
