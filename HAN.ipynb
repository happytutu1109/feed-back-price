{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mlt\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数目： 15593\n"
     ]
    }
   ],
   "source": [
    "def data_loading(data_dir):\n",
    "    data, labels, ids= [], [], []\n",
    "    df = pd.read_csv(data_dir)\n",
    "    data_list = df.loc[:,'discourse_text'].values\n",
    "    label_list_ = df.loc[:,'discourse_type'].values\n",
    "    id_list = df.loc[:,'id'].values\n",
    "    type_dict = {'Lead':1,'Position':2,'Claim':3,'Counterclaim':4,'Rebuttal':5,'Evidence':6,'Concluding Statement':7}\n",
    "    label_list = []\n",
    "    for i in label_list_:\n",
    "        label_list.append(type_dict[i])\n",
    "    if len(data_list)!=len(label_list):\n",
    "        return 'length bug'\n",
    "    n = len(data_list)\n",
    "    for data_ in range(n):\n",
    "        sentence_split = data_list[data_].split(\".\")[:-1] if data_list[data_].split('.')[-1] == str('') else data_list[data_].split(\".\")\n",
    "        label_split = [label_list[data_] for i in range(len(sentence_split))]\n",
    "        id_split = [id_list[data_] for i in range(len(sentence_split))]\n",
    "        for j in range(len(sentence_split)):\n",
    "            if sentence_split[j]!=' ':\n",
    "                data.append(sentence_split[j].lower()) \n",
    "                labels.append(label_split[j])\n",
    "                ids.append(id_split[j])\n",
    "    data_article, labels_article = [],[]\n",
    "    data_sentence, labels_sentence = [],[]\n",
    "    for i in range(len(ids)-1):\n",
    "        if ids[i]==ids[i+1]: \n",
    "            data_sentence.append(data[i])\n",
    "            labels_sentence.append(labels[i])\n",
    "        else:\n",
    "            data_article.append(data_sentence)\n",
    "            labels_article.append(labels_sentence)\n",
    "            data_sentence,labels_sentence = [],[]\n",
    "    return data_article , labels_article\n",
    "\n",
    "\n",
    "train_data = data_loading('./train.csv')\n",
    "print('训练集数目：', len(train_data[0]))\n",
    "#print(train_data[0][0],train_data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(train_data[1][i]) for i in range(len(train_data[1]))],bins = 10,rwidth=1, range=(1,80))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article,labels_ = train_data[0],train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [tensor([[  101,  2715,  4286,  2651,  2024,  2467,  2006,  2037,  3042,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2024,  2467,  2006,  2037,  3042,  2062,  2084,  1019,\n",
      "          2847,  1037,  2154,  2053,  2644,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2035,  2027,  2079,  2003,  3793,  2067,  1998,  2830,  1998,\n",
      "          2074,  2031,  2177, 11834,  2015,  2006,  2591,  2865,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2130,  2079,  2009,  2096,  4439,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2024,  2070,  2428,  2919,  8465,  2043,  4933,  6433,\n",
      "          2043,  2009,  3310,  2000,  1037,  3042,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2070,  3056,  2752,  1999,  1996,  2142,  2163,  7221, 11640,\n",
      "          2013,  2465,  4734,  2074,  2138,  1997,  2009,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2043,  2111,  2031, 11640,  1010,  2027,  2113,  2055,  3056,\n",
      "         18726,  2008,  2027,  2031,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 18726,  2066,  9130, 10474, 16021, 23091,  1998, 10245,  7507,\n",
      "          2102,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2061,  2066,  2065,  1037,  2767,  5829,  2185,  1998,  2017,\n",
      "          2215,  2000,  2022,  1999,  3967,  2017,  2064,  2145,  2022,  1999,\n",
      "          3967,  2011, 14739,  6876,  2030,  3793,  7696,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2111,  2467,  2031,  2367,  3971,  2129,  2000, 10639,  2007,\n",
      "          1037,  3042,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 11640,  2031,  2904,  2349,  2000,  2256,  4245,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  4439,  2003,  2028,  1997,  1996,  2126,  2129,  2000,  2131,\n",
      "          2105,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2111,  2467,  2022,  2006,  2037, 11640,  2096,  2725,  2009,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2029,  2064,  3426,  3809,  3471,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2008,  1005,  1055,  2339,  2045,  1005,  1055,  1037,  2518,\n",
      "          2008,  1005,  1055,  2170,  2053,  3793,  2075,  2096,  4439,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2008,  1005,  1055,  1037,  2428,  2590,  2518,  2000,  3342,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2070,  2111,  2145,  2079,  2009,  2138,  2027,  2228,  2009,\n",
      "          1005,  1055,  5236,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2053,  3043,  2054,  2027,  2079,  2027,  2145,  2031,  2000,\n",
      "         15470,  2009,  2138,  2008,  1005,  1055,  1996,  2069,  2126,  2129,\n",
      "          2106,  2002,  3828,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2823,  2006,  1996,  2739,  2045,  2003,  2593,  2019,  4926,\n",
      "          2030,  1037,  5920,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2009,  2453,  9125,  2619,  2025,  2559,  2073,  2027,  1005,\n",
      "          2128,  2183,  2030,  1056, 28394,  2102,  2008,  2619,  2741,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2009,  2593,  4544,  2030,  2331,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2065,  1037,  8075,  2193,  2758,  1045,  1005,  1049,  2183,\n",
      "          2000,  3102,  2017,  1998,  2027,  2113,  2073,  2017,  2444,  2021,\n",
      "          2017,  2123,  1005,  1056,  2113,  1996,  2711,  1005,  1055,  3967,\n",
      "          1010,   102],\n",
      "        [  101,  2029,  2064,  2203,  2039,  2428,  6649,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 11640,  2024,  2986,  2000,  2224,  1998,  2009,  1005,  1055,\n",
      "          2036,  1996,  2190,  2126,  2000,  2272,  2058,  2393,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2065,  2017,  2175,  2083,  1037,  3291,  1998,  2017,  2064,\n",
      "          1005,  1056,  2424,  2393,  2017,  1010,  2467,  2031,  1037,  3042,\n",
      "          2045,  2007,  2017,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2130,  2295, 11640,  2024,  2109,  2471,  2296,  2154,  2004,\n",
      "          2146,  2004,  2017,  1005,  2128,  3647,  2009,  2052,  2272,  2046,\n",
      "          2224,  2065,  2017,  2131,  2046,  4390,   102,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2191,  2469,  2017,  2079,  2025,  2022,  2066,  2023,  3042,\n",
      "          2096,  2017,  1005,  2128,  1999,  1996,  2690,  1997,  4439,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1996,  2739,  2467,  7172,  2043,  2111,  2079,  2242,  5236,\n",
      "          2105,  2008,  7336,  2037, 11640,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), tensor([[  101,  6853,  2323,  2025,  2022,  2583,  2000,  2224, 11640,  2096,\n",
      "          4082,  1037,  4316,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2040,  2109,  2037,  3042,  2096,  4082,  1037,  4316,\n",
      "          2024,  2087,  3497,  2000,  2131,  2046,  2019,  4926,  2008,  2071,\n",
      "          2022, 10611,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  2019,  3720,  2011,  1996,  9586, 17840,  3813,\n",
      "          1010,  2538,  1003,  1997, 13496,  2008,  2020,  2112,  1997,  1037,\n",
      "         10611,  2482,  4926,  2001,  2349,  2000, 11640,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  1996,  2168,  3720,  1010,  3486,  1003,  2113,\n",
      "          1996,  3891,  2021,  3613,  2478,  2037, 11640,  2096,  2006,  1996,\n",
      "          2346,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2023,  3065,  2008,  2049,  3458,  4795,  1998, 20868,  6072,\n",
      "         26029, 19307,  1997,  6853,  2025,  2000,  2022,  3929,  5204,  1997,\n",
      "          2037, 11301,  2096,  4439,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2323,  2022,  2583,  2000, 10152,  2302,  2151, 14836,\n",
      "          2015,  1010,  2138,  2009,  2071,  2022, 10611,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  2178,  3720,  1010,  1000, 11116,  4439,  1000,\n",
      "          2011,  1996, 18699, 27110,  1010,  2045,  2038,  2525,  2042,  2055,\n",
      "          1017,  1010,  2199,  3042,  3141,  2482,  4926,  6677,  2144,  2418,\n",
      "           102,     0],\n",
      "        [  101,  1996,  3720,  2163,  2008,  9458,  2131,  2205, 11116,  2007,\n",
      "          2037, 11640,  1010,  2029,  5320,  2037,  4926,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 13436,  2008,  2064,  2022,  4089,  9511,  2011,  7995,  2006,\n",
      "          1996,  2346,  1998,  2025,  1037,  3042,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2323,  2025,  2022,  2583,  2000,  2224,  2037, 11640,\n",
      "          2012,  2035,  2096,  4439,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1999,  7091,  1010,  6853,  2323,  2025,  2583,  2000,  2147,\n",
      "          1037,  4316,  2096,  2478,  2037,  3526,  3042,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2040,  3594,  2037, 11640,  2096,  4082,  1037,  4316,\n",
      "          1998,  2024,  3497,  2000,  2031,  2019,  4926,  2059,  2216,  2040,\n",
      "          2123,  1005,  1056,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])] [tensor([1, 1, 1, 1, 2, 6, 6, 6, 6, 6, 6, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3,\n",
      "        6, 6, 6, 7]), tensor([2, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7])]\n",
      "[tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])]\n",
      "[tensor([1, 1, 1, 2, 3, 6, 6, 6, 4, 4, 5, 5, 7]), tensor([1, 1, 1, 2, 3, 3, 3, 3, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6,\n",
      "        6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7])]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_art = []\n",
    "attention_masks_art = []\n",
    "labels = []\n",
    "# For every sentence...\n",
    "for art in range(len(article)):\n",
    "    input_ids_sent = []\n",
    "    attention_masks_sent = []\n",
    "    for sent in article[art]:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 32,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids_sent.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_sent.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "\n",
    "    input_ids_sent = torch.cat(input_ids_sent, dim=0)\n",
    "    attention_masks_sent = torch.cat(attention_masks_sent, dim=0)\n",
    "    labels_sent = torch.tensor(labels_[art])\n",
    "\n",
    "    input_ids_art.append(input_ids_sent)\n",
    "    attention_masks_art.append(attention_masks_sent)\n",
    "    labels.append(labels_sent)\n",
    "\n",
    "#labels = [label.tolist() for label in labels]\n",
    "\n",
    "#print('Original: ', article[0])\n",
    "print('Token IDs:', input_ids_art[:2],labels[:2])\n",
    "print(attention_masks_art[:2])\n",
    "print(labels[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对文章层面进行padding，并记录文章有效句子个数\n",
    "def padding(id,mask,label,max_sent_num=50,max_sent_len=32):     #确保输入的lst为list列表结构，其中每一个元素为pytorchtensor。max_sent_len需要与前面tokenizer里参数对应上\n",
    "    valid_num = []\n",
    "    for i in range(len(id)):\n",
    "        id[i] = id[i].tolist()\n",
    "        mask[i] = mask[i].tolist()\n",
    "        label[i] = label[i].tolist()\n",
    "        valid_num.append(len(id[i]) if len(id[i])<=50 else 50)\n",
    "        if len(id[i])<max_sent_num:\n",
    "            id[i] = id[i]+[[101,102]+[0 for _ in range(max_sent_len-2)] for __ in range(max_sent_num-len(id[i]))]\n",
    "            mask[i] = mask[i]+[[1,1]+[0 for _ in range(max_sent_len-2)] for __ in range(max_sent_num-len(mask[i]))]\n",
    "            label[i] = label[i] + [0 for _ in range(max_sent_num-len(label[i]))]\n",
    "        if len(id[i])>max_sent_num:\n",
    "            id[i] = id[i][:20]+id[i][-30:]\n",
    "            mask[i] = mask[i][:20]+id[i][-30:]\n",
    "            label[i] = label[i][:20]+label[i][-30:]\n",
    "    #return id,mask,valid_num\n",
    "    return torch.tensor(id),torch.tensor(mask),torch.tensor(label),valid_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_art,attention_masks_art,labels,valid_num = padding(input_ids_art,attention_masks_art,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15593, 50, 32]),\n",
       " torch.Size([15593, 50, 32]),\n",
       " torch.Size([15593, 50]),\n",
       " 15593)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids_art.size(),attention_masks_art.size(),labels.size(),len(valid_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class article_dataset(Dataset):\n",
    "    def __init__(self,input,masks,labels,valid_num):\n",
    "        super(article_dataset,self).__init__()\n",
    "        self.input = input\n",
    "        self.labels = labels\n",
    "        self.masks = masks\n",
    "        self.valid_num = valid_num\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input[idx],self.masks[idx],self.labels[idx],self.valid_num[idx]      #input和masks维度为sentence_num,sentence_len，label维度 sentence_num\n",
    "    def __len__(self):                                                                 #valid_num为int\n",
    "        return len(self.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章中句子个数不一致问题暂定解决办法：\n",
    "    1：找到合适的sentence_num（尽量所有文章的长度均小于该值）。\n",
    "    2：对小于该长度的文章，在处理完tokenizer之前进行空句子补齐，并记录其有效长度。（其会在tokennizer阶段被标注为[101,102,...,0]）。\n",
    "    3：所有用于补齐的空句子不设置类别，在处理空句子时记录每篇文章的有效句子长度。句子有效长度在每一个batch中使用一个list储存（想办法在dataset中实现）\n",
    "    4：在模型框架中，仅输出有效句子的损失并进行后向传播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = article_dataset(input_ids_art,attention_masks_art,labels,valid_num)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset,sampler=train_sampler,batch_size=2,drop_last=True)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "val_dataloader = DataLoader(val_dataset,sampler=val_sampler,batch_size=2,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_resnetblock(nn.Module):\n",
    "    def __init__(self,input_size,output_size,drop=0.1):\n",
    "        super(linear_resnetblock,self).__init__()\n",
    "        self.res = nn.Sequential(\n",
    "            nn.Linear(input_size,output_size),\n",
    "            nn.Dropout(drop),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(output_size)\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self,x):\n",
    "        out = self.res(x)\n",
    "        return self.relu(out+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAN(nn.Module):\n",
    "    def __init__(self,bert_name,hidden_size_rnn,num_layers_rnn,classes,dropout):     #valid_num表示该input中有效的sentence个数\n",
    "        super(HAN,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_name)\n",
    "        hidden_size_bert = self.bert.config.hidden_size\n",
    "        self.rnn = nn.LSTM(input_size=hidden_size_bert, hidden_size=hidden_size_rnn, num_layers=num_layers_rnn, bias=True, dropout=dropout,batch_first=True)\n",
    "        #self.outlayer = nn.Linear(hidden_size_rnn,classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.classes = classes\n",
    "        self.outlayer = nn.Sequential(nn.Linear(hidden_size_rnn,512),\n",
    "                                        nn.Dropout(0.5),\n",
    "                                        linear_resnetblock(512,512),\n",
    "                                        linear_resnetblock(512,512),\n",
    "                                        linear_resnetblock(512,512),\n",
    "                                        linear_resnetblock(512,512),\n",
    "                                        linear_resnetblock(512,512),\n",
    "                                        linear_resnetblock(512,512),\n",
    "                                        linear_resnetblock(512,512),\n",
    "                                        nn.ELU(),\n",
    "                                        nn.Dropout(0.5),\n",
    "                                        nn.Linear(512,256),\n",
    "                                        nn.ELU(),\n",
    "                                        nn.Linear(256,128),\n",
    "                                        nn.ELU(),\n",
    "                                        nn.Linear(128,classes))\n",
    "                                        \n",
    "        \n",
    "    def forward(self,input_ids,input_masks,valid_num,labels=None):  #input维度 batchsize,num_sentence,sent_len , label维度batchsize,num_sentence,sent_len\n",
    "        batch_size,sentence_num,sentence_len = input_ids.size(0),input_ids.size(1),input_ids.size(2)\n",
    "        hidden_size_bert = self.bert.config.hidden_size\n",
    "        input_ids,input_masks = input_ids.view(-1,sentence_len),input_masks.view(-1,sentence_len)\n",
    "        out = self.bert(input_ids,input_masks)      #out维度 batchsize*num_sentence,sen_len,hidden_size\n",
    "        out = out[0][:,0,:]     #out维度 batchsize*num_sentence,hidden_size\n",
    "        #out = out.view(batch_size,sentence_num,hidden_size_bert)    #out维度转化为batchsize,num_sentence,hidden_size\n",
    "        out = out.view(batch_size,sentence_num,hidden_size_bert).float()\n",
    "        self.rnn.flatten_parameters()\n",
    "        outputs,_ = self.rnn(out)   #outputs维度 batchsize,num_sentence,hidden_size\n",
    "        outputs = self.outlayer(outputs)    #outputs维度为 batchsize,num_sentence,classes\n",
    "        output = []\n",
    "        for batch in range(len(outputs)):\n",
    "            output.append(outputs[batch][0:valid_num[batch],:].tolist())\n",
    "        if labels is not None:\n",
    "            outputs_loss,labels = outputs.view(-1,self.classes),labels.view(-1)\n",
    "            #outputs = torch.softmax(outputs)\n",
    "            loss = self.loss(outputs_loss,labels)\n",
    "            return loss,outputs\n",
    "        else:\n",
    "            return 0,outputs\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HAN(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (rnn): LSTM(768, 256, batch_first=True, dropout=0.1)\n",
       "  (loss): CrossEntropyLoss()\n",
       "  (outlayer): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): linear_resnetblock(\n",
       "      (res): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): linear_resnetblock(\n",
       "      (res): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): linear_resnetblock(\n",
       "      (res): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): linear_resnetblock(\n",
       "      (res): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (6): linear_resnetblock(\n",
       "      (res): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (7): linear_resnetblock(\n",
       "      (res): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (8): linear_resnetblock(\n",
       "      (res): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (9): ELU(alpha=1.0)\n",
       "    (10): Dropout(p=0.5, inplace=False)\n",
       "    (11): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (12): ELU(alpha=1.0)\n",
       "    (13): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (14): ELU(alpha=1.0)\n",
       "    (15): Linear(in_features=128, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HAN('bert-base-uncased',256,1,8,0.1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 4e-5\n",
    "optimizer_grouped_parameters = [\n",
    "{'params': [p for n, p in model.named_parameters() if 'bert' in n],'lr':lr},\n",
    "{'params': [p for n, p in model.named_parameters() if 'bert' not in n], 'lr' : 30*lr}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters,lr=lr, eps=1e-8)\n",
    "#optimizer = AdamW(model.parameters(), lr = 2e-5, eps = 1e-8)\n",
    "epochs = 2\n",
    "# training steps 的数量: [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# 设计 learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0.1*total_steps, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_acc(preds, labels,classes):     \n",
    "    preds = preds.view(-1,classes)\n",
    "    labels = labels.view(-1)\n",
    "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float()      #eq里面的两个参数的shape=torch.Size([16])    \n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc_eva(preds, labels,classes,valid,batchsize=2):     #仅限batchsize为2时使用\n",
    "    \n",
    "    preds = torch.cat([preds[0][:valid[0]],preds[1][:valid[1]]],dim=0)\n",
    "\n",
    "    labels = torch.cat([labels[0][:valid[0]],labels[1][:valid[1]]],dim=0)\n",
    "    correct = torch.eq(torch.max(preds, dim=1)[1], labels.flatten()).float()      #eq里面的两个参数的shape=torch.Size([16])    \n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(elapsed):    \n",
    "    elapsed_rounded = int(round((elapsed)))    \n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer):\n",
    "    t0 = time.time()\n",
    "    avg_loss, avg_acc = [],[]   \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # 每隔40个batch 输出一下所用时间.\n",
    "            if step % 40 == 0 and not step == 0:            \n",
    "                elapsed = format_time(time.time() - t0)             \n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step+epoch*len(train_dataloader), len(train_dataloader)*epochs, elapsed))\n",
    "                print(np.array(avg_loss).mean())\n",
    "                avg_loss, avg_acc = [],[]\n",
    "            input_id,input_mask,label,valid_num_tensor = batch[0].long().to(device),batch[1].long().to(device),batch[2].long().to(device),batch[3]\n",
    "            valid_num = valid_num_tensor.tolist()\n",
    "            output = model(input_id,input_mask,valid_num,label)\n",
    "            loss,outputs = output[0],output[1]\n",
    "            avg_loss.append(loss.item())\n",
    "        \n",
    "            acc = binary_acc(outputs, label,classes=8)\n",
    "            avg_acc.append(acc)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), 1.0)      #大于1的梯度将其设为1.0, 以防梯度爆炸\n",
    "            optimizer.step()              #更新模型参数\n",
    "            scheduler.step()              #更新learning rate\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    avg_loss = np.array(avg_loss).mean()\n",
    "    return avg_loss, avg_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch    40  of  14,032.    Elapsed: 0:00:10.\n",
      "1.6898651361465453\n",
      "  Batch    80  of  14,032.    Elapsed: 0:00:19.\n",
      "0.7906016953289509\n",
      "  Batch   120  of  14,032.    Elapsed: 0:00:29.\n",
      "0.6434898242354393\n",
      "  Batch   160  of  14,032.    Elapsed: 0:00:38.\n",
      "0.6921609029173851\n",
      "  Batch   200  of  14,032.    Elapsed: 0:00:48.\n",
      "0.6295232079923153\n",
      "  Batch   240  of  14,032.    Elapsed: 0:00:58.\n",
      "0.587821564078331\n",
      "  Batch   280  of  14,032.    Elapsed: 0:01:07.\n",
      "0.548839557915926\n",
      "  Batch   320  of  14,032.    Elapsed: 0:01:17.\n",
      "0.5763557136058808\n",
      "  Batch   360  of  14,032.    Elapsed: 0:01:27.\n",
      "0.5609979096800088\n",
      "  Batch   400  of  14,032.    Elapsed: 0:01:37.\n",
      "0.5687833651900291\n",
      "  Batch   440  of  14,032.    Elapsed: 0:01:46.\n",
      "0.5520826712250709\n",
      "  Batch   480  of  14,032.    Elapsed: 0:01:56.\n",
      "0.5241652742028237\n",
      "  Batch   520  of  14,032.    Elapsed: 0:02:06.\n",
      "0.5743068505078555\n",
      "  Batch   560  of  14,032.    Elapsed: 0:02:15.\n",
      "0.4613726494833827\n",
      "  Batch   600  of  14,032.    Elapsed: 0:02:25.\n",
      "0.48537379316985607\n",
      "  Batch   640  of  14,032.    Elapsed: 0:02:35.\n",
      "0.47707870677113534\n",
      "  Batch   680  of  14,032.    Elapsed: 0:02:45.\n",
      "0.490018879622221\n",
      "  Batch   720  of  14,032.    Elapsed: 0:02:54.\n",
      "0.4647738516330719\n",
      "  Batch   760  of  14,032.    Elapsed: 0:03:04.\n",
      "0.4604361917823553\n",
      "  Batch   800  of  14,032.    Elapsed: 0:03:14.\n",
      "0.5474992237985135\n",
      "  Batch   840  of  14,032.    Elapsed: 0:03:23.\n",
      "0.5236832737922669\n",
      "  Batch   880  of  14,032.    Elapsed: 0:03:33.\n",
      "0.4773015346378088\n",
      "  Batch   920  of  14,032.    Elapsed: 0:03:43.\n",
      "0.44866362437605856\n",
      "  Batch   960  of  14,032.    Elapsed: 0:03:53.\n",
      "0.488722163438797\n",
      "  Batch 1,000  of  14,032.    Elapsed: 0:04:02.\n",
      "0.4544945452362299\n",
      "  Batch 1,040  of  14,032.    Elapsed: 0:04:12.\n",
      "0.5180209670215845\n",
      "  Batch 1,080  of  14,032.    Elapsed: 0:04:22.\n",
      "0.43478258401155473\n",
      "  Batch 1,120  of  14,032.    Elapsed: 0:04:32.\n",
      "0.42643488496541976\n",
      "  Batch 1,160  of  14,032.    Elapsed: 0:04:41.\n",
      "0.459015104919672\n",
      "  Batch 1,200  of  14,032.    Elapsed: 0:04:51.\n",
      "0.5002793844789266\n",
      "  Batch 1,240  of  14,032.    Elapsed: 0:05:01.\n",
      "0.4770772036164999\n",
      "  Batch 1,280  of  14,032.    Elapsed: 0:05:11.\n",
      "0.460067805275321\n",
      "  Batch 1,320  of  14,032.    Elapsed: 0:05:20.\n",
      "0.4032608434557915\n",
      "  Batch 1,360  of  14,032.    Elapsed: 0:05:30.\n",
      "0.42990730591118337\n",
      "  Batch 1,400  of  14,032.    Elapsed: 0:05:40.\n",
      "0.4460956215858459\n",
      "  Batch 1,440  of  14,032.    Elapsed: 0:05:49.\n",
      "0.40703850854188206\n",
      "  Batch 1,480  of  14,032.    Elapsed: 0:05:59.\n",
      "0.4693783104419708\n",
      "  Batch 1,520  of  14,032.    Elapsed: 0:06:09.\n",
      "0.43907790053635837\n",
      "  Batch 1,560  of  14,032.    Elapsed: 0:06:19.\n",
      "0.4691447980701923\n",
      "  Batch 1,600  of  14,032.    Elapsed: 0:06:28.\n",
      "0.4232133042067289\n",
      "  Batch 1,640  of  14,032.    Elapsed: 0:06:38.\n",
      "0.4101168796420097\n",
      "  Batch 1,680  of  14,032.    Elapsed: 0:06:48.\n",
      "0.43603843823075294\n",
      "  Batch 1,720  of  14,032.    Elapsed: 0:06:58.\n",
      "0.432481737062335\n",
      "  Batch 1,760  of  14,032.    Elapsed: 0:07:07.\n",
      "0.4429614689201117\n",
      "  Batch 1,800  of  14,032.    Elapsed: 0:07:17.\n",
      "0.48082834519445894\n",
      "  Batch 1,840  of  14,032.    Elapsed: 0:07:27.\n",
      "0.4358181796967983\n",
      "  Batch 1,880  of  14,032.    Elapsed: 0:07:37.\n",
      "0.34673955496400594\n",
      "  Batch 1,920  of  14,032.    Elapsed: 0:07:46.\n",
      "0.41943434365093707\n",
      "  Batch 1,960  of  14,032.    Elapsed: 0:07:56.\n",
      "0.425349329225719\n",
      "  Batch 2,000  of  14,032.    Elapsed: 0:08:06.\n",
      "0.4471916975453496\n",
      "  Batch 2,040  of  14,032.    Elapsed: 0:08:15.\n",
      "0.37357581295073033\n",
      "  Batch 2,080  of  14,032.    Elapsed: 0:08:25.\n",
      "0.41773723997175694\n",
      "  Batch 2,120  of  14,032.    Elapsed: 0:08:35.\n",
      "0.40352400839328767\n",
      "  Batch 2,160  of  14,032.    Elapsed: 0:08:45.\n",
      "0.40676247887313366\n",
      "  Batch 2,200  of  14,032.    Elapsed: 0:08:54.\n",
      "0.3656601462513208\n",
      "  Batch 2,240  of  14,032.    Elapsed: 0:09:04.\n",
      "0.3810508340597153\n",
      "  Batch 2,280  of  14,032.    Elapsed: 0:09:14.\n",
      "0.3859965980052948\n",
      "  Batch 2,320  of  14,032.    Elapsed: 0:09:24.\n",
      "0.39121567979454996\n",
      "  Batch 2,360  of  14,032.    Elapsed: 0:09:33.\n",
      "0.4059824518859386\n",
      "  Batch 2,400  of  14,032.    Elapsed: 0:09:43.\n",
      "0.366081265360117\n",
      "  Batch 2,440  of  14,032.    Elapsed: 0:09:53.\n",
      "0.42868287302553654\n",
      "  Batch 2,480  of  14,032.    Elapsed: 0:10:02.\n",
      "0.3879305124282837\n",
      "  Batch 2,520  of  14,032.    Elapsed: 0:10:12.\n",
      "0.4022726096212864\n",
      "  Batch 2,560  of  14,032.    Elapsed: 0:10:22.\n",
      "0.40376389361917975\n",
      "  Batch 2,600  of  14,032.    Elapsed: 0:10:32.\n",
      "0.4077093042433262\n",
      "  Batch 2,640  of  14,032.    Elapsed: 0:10:41.\n",
      "0.3632817599922419\n",
      "  Batch 2,680  of  14,032.    Elapsed: 0:10:51.\n",
      "0.3997566856443882\n",
      "  Batch 2,720  of  14,032.    Elapsed: 0:11:01.\n",
      "0.3497965205460787\n",
      "  Batch 2,760  of  14,032.    Elapsed: 0:11:11.\n",
      "0.39635474793612957\n",
      "  Batch 2,800  of  14,032.    Elapsed: 0:11:20.\n",
      "0.4095442395657301\n",
      "  Batch 2,840  of  14,032.    Elapsed: 0:11:30.\n",
      "0.3480977389961481\n",
      "  Batch 2,880  of  14,032.    Elapsed: 0:11:40.\n",
      "0.4054279576987028\n",
      "  Batch 2,920  of  14,032.    Elapsed: 0:11:49.\n",
      "0.3641536571085453\n",
      "  Batch 2,960  of  14,032.    Elapsed: 0:11:59.\n",
      "0.39162371307611465\n",
      "  Batch 3,000  of  14,032.    Elapsed: 0:12:09.\n",
      "0.3687302691861987\n",
      "  Batch 3,040  of  14,032.    Elapsed: 0:12:19.\n",
      "0.3612777814269066\n",
      "  Batch 3,080  of  14,032.    Elapsed: 0:12:28.\n",
      "0.3636609610170126\n",
      "  Batch 3,120  of  14,032.    Elapsed: 0:12:38.\n",
      "0.36265130192041395\n",
      "  Batch 3,160  of  14,032.    Elapsed: 0:12:48.\n",
      "0.4516443260014057\n",
      "  Batch 3,200  of  14,032.    Elapsed: 0:12:58.\n",
      "0.3651694579049945\n",
      "  Batch 3,240  of  14,032.    Elapsed: 0:13:07.\n",
      "0.39404113553464415\n",
      "  Batch 3,280  of  14,032.    Elapsed: 0:13:17.\n",
      "0.36126054357737303\n",
      "  Batch 3,320  of  14,032.    Elapsed: 0:13:27.\n",
      "0.3326114736497402\n",
      "  Batch 3,360  of  14,032.    Elapsed: 0:13:37.\n",
      "0.33330243453383446\n",
      "  Batch 3,400  of  14,032.    Elapsed: 0:13:46.\n",
      "0.3308011509478092\n",
      "  Batch 3,440  of  14,032.    Elapsed: 0:13:56.\n",
      "0.33311381805688145\n",
      "  Batch 3,480  of  14,032.    Elapsed: 0:14:06.\n",
      "0.38082618210464714\n",
      "  Batch 3,520  of  14,032.    Elapsed: 0:14:16.\n",
      "0.30693264696747063\n",
      "  Batch 3,560  of  14,032.    Elapsed: 0:14:25.\n",
      "0.3275951033458114\n",
      "  Batch 3,600  of  14,032.    Elapsed: 0:14:35.\n",
      "0.37677767891436814\n",
      "  Batch 3,640  of  14,032.    Elapsed: 0:14:45.\n",
      "0.35466308165341615\n",
      "  Batch 3,680  of  14,032.    Elapsed: 0:14:54.\n",
      "0.35801932476460935\n",
      "  Batch 3,720  of  14,032.    Elapsed: 0:15:04.\n",
      "0.34242513719946144\n",
      "  Batch 3,760  of  14,032.    Elapsed: 0:15:14.\n",
      "0.3538331162184477\n",
      "  Batch 3,800  of  14,032.    Elapsed: 0:15:24.\n",
      "0.35081823002547025\n",
      "  Batch 3,840  of  14,032.    Elapsed: 0:15:33.\n",
      "0.37149093486368656\n",
      "  Batch 3,880  of  14,032.    Elapsed: 0:15:43.\n",
      "0.3276710782200098\n",
      "  Batch 3,920  of  14,032.    Elapsed: 0:15:53.\n",
      "0.3975538482889533\n",
      "  Batch 3,960  of  14,032.    Elapsed: 0:16:03.\n",
      "0.30705039482563734\n",
      "  Batch 4,000  of  14,032.    Elapsed: 0:16:12.\n",
      "0.35745094195008276\n",
      "  Batch 4,040  of  14,032.    Elapsed: 0:16:22.\n",
      "0.3645507203415036\n",
      "  Batch 4,080  of  14,032.    Elapsed: 0:16:32.\n",
      "0.2852524150162935\n",
      "  Batch 4,120  of  14,032.    Elapsed: 0:16:41.\n",
      "0.33766407091170547\n",
      "  Batch 4,160  of  14,032.    Elapsed: 0:16:51.\n",
      "0.378378414362669\n",
      "  Batch 4,200  of  14,032.    Elapsed: 0:17:01.\n",
      "0.311760313436389\n",
      "  Batch 4,240  of  14,032.    Elapsed: 0:17:11.\n",
      "0.3475100763142109\n",
      "  Batch 4,280  of  14,032.    Elapsed: 0:17:20.\n",
      "0.35038741808384655\n",
      "  Batch 4,320  of  14,032.    Elapsed: 0:17:30.\n",
      "0.41115855034440757\n",
      "  Batch 4,360  of  14,032.    Elapsed: 0:17:40.\n",
      "0.3519929386675358\n",
      "  Batch 4,400  of  14,032.    Elapsed: 0:17:50.\n",
      "0.3548680616542697\n",
      "  Batch 4,440  of  14,032.    Elapsed: 0:17:59.\n",
      "0.32740608751773836\n",
      "  Batch 4,480  of  14,032.    Elapsed: 0:18:09.\n",
      "0.32368608079850675\n",
      "  Batch 4,520  of  14,032.    Elapsed: 0:18:19.\n",
      "0.3364363728091121\n",
      "  Batch 4,560  of  14,032.    Elapsed: 0:18:29.\n",
      "0.3648459672927856\n",
      "  Batch 4,600  of  14,032.    Elapsed: 0:18:38.\n",
      "0.3016298979520798\n",
      "  Batch 4,640  of  14,032.    Elapsed: 0:18:48.\n",
      "0.3505623763427138\n",
      "  Batch 4,680  of  14,032.    Elapsed: 0:18:58.\n",
      "0.3552379563450813\n",
      "  Batch 4,720  of  14,032.    Elapsed: 0:19:07.\n",
      "0.2916470704600215\n",
      "  Batch 4,760  of  14,032.    Elapsed: 0:19:17.\n",
      "0.35375625379383563\n",
      "  Batch 4,800  of  14,032.    Elapsed: 0:19:27.\n",
      "0.36833258643746375\n",
      "  Batch 4,840  of  14,032.    Elapsed: 0:19:37.\n",
      "0.35021855477243663\n",
      "  Batch 4,880  of  14,032.    Elapsed: 0:19:46.\n",
      "0.32539652548730374\n",
      "  Batch 4,920  of  14,032.    Elapsed: 0:19:56.\n",
      "0.30980441719293594\n",
      "  Batch 4,960  of  14,032.    Elapsed: 0:20:06.\n",
      "0.37291929572820665\n",
      "  Batch 5,000  of  14,032.    Elapsed: 0:20:15.\n",
      "0.32262598127126696\n",
      "  Batch 5,040  of  14,032.    Elapsed: 0:20:25.\n",
      "0.34026705035939814\n",
      "  Batch 5,080  of  14,032.    Elapsed: 0:20:35.\n",
      "0.319163647852838\n",
      "  Batch 5,120  of  14,032.    Elapsed: 0:20:45.\n",
      "0.346086161583662\n",
      "  Batch 5,160  of  14,032.    Elapsed: 0:20:54.\n",
      "0.34793292097747325\n",
      "  Batch 5,200  of  14,032.    Elapsed: 0:21:04.\n",
      "0.33531918134540317\n",
      "  Batch 5,240  of  14,032.    Elapsed: 0:21:14.\n",
      "0.3197395529597998\n",
      "  Batch 5,280  of  14,032.    Elapsed: 0:21:24.\n",
      "0.3420476758852601\n",
      "  Batch 5,320  of  14,032.    Elapsed: 0:21:33.\n",
      "0.40236295089125634\n",
      "  Batch 5,360  of  14,032.    Elapsed: 0:21:43.\n",
      "0.3349727002903819\n",
      "  Batch 5,400  of  14,032.    Elapsed: 0:21:53.\n",
      "0.34075737949460744\n",
      "  Batch 5,440  of  14,032.    Elapsed: 0:22:02.\n",
      "0.3062139168381691\n",
      "  Batch 5,480  of  14,032.    Elapsed: 0:22:12.\n",
      "0.311732180044055\n",
      "  Batch 5,520  of  14,032.    Elapsed: 0:22:22.\n",
      "0.3119754668325186\n",
      "  Batch 5,560  of  14,032.    Elapsed: 0:22:32.\n",
      "0.32111906372010707\n",
      "  Batch 5,600  of  14,032.    Elapsed: 0:22:41.\n",
      "0.3464762447401881\n",
      "  Batch 5,640  of  14,032.    Elapsed: 0:22:51.\n",
      "0.289844935387373\n",
      "  Batch 5,680  of  14,032.    Elapsed: 0:23:01.\n",
      "0.3351243084296584\n",
      "  Batch 5,720  of  14,032.    Elapsed: 0:23:11.\n",
      "0.36689943410456183\n",
      "  Batch 5,760  of  14,032.    Elapsed: 0:23:20.\n",
      "0.3227447599172592\n",
      "  Batch 5,800  of  14,032.    Elapsed: 0:23:30.\n",
      "0.3042946748435497\n",
      "  Batch 5,840  of  14,032.    Elapsed: 0:23:40.\n",
      "0.31404480431228876\n",
      "  Batch 5,880  of  14,032.    Elapsed: 0:23:49.\n",
      "0.3095669489353895\n",
      "  Batch 5,920  of  14,032.    Elapsed: 0:23:59.\n",
      "0.3220450609922409\n",
      "  Batch 5,960  of  14,032.    Elapsed: 0:24:09.\n",
      "0.3041622398421168\n",
      "  Batch 6,000  of  14,032.    Elapsed: 0:24:19.\n",
      "0.304838745854795\n",
      "  Batch 6,040  of  14,032.    Elapsed: 0:24:28.\n",
      "0.3259495092555881\n",
      "  Batch 6,080  of  14,032.    Elapsed: 0:24:38.\n",
      "0.38693844228982927\n",
      "  Batch 6,120  of  14,032.    Elapsed: 0:24:48.\n",
      "0.34342063404619694\n",
      "  Batch 6,160  of  14,032.    Elapsed: 0:24:57.\n",
      "0.3187410067766905\n",
      "  Batch 6,200  of  14,032.    Elapsed: 0:25:07.\n",
      "0.3366395205259323\n",
      "  Batch 6,240  of  14,032.    Elapsed: 0:25:17.\n",
      "0.294978279620409\n",
      "  Batch 6,280  of  14,032.    Elapsed: 0:25:27.\n",
      "0.3242356427013874\n",
      "  Batch 6,320  of  14,032.    Elapsed: 0:25:36.\n",
      "0.32847610488533974\n",
      "  Batch 6,360  of  14,032.    Elapsed: 0:25:46.\n",
      "0.3253452945500612\n",
      "  Batch 6,400  of  14,032.    Elapsed: 0:25:56.\n",
      "0.33868194511160254\n",
      "  Batch 6,440  of  14,032.    Elapsed: 0:26:06.\n",
      "0.3057771634310484\n",
      "  Batch 6,480  of  14,032.    Elapsed: 0:26:15.\n",
      "0.3495595008134842\n",
      "  Batch 6,520  of  14,032.    Elapsed: 0:26:25.\n",
      "0.30697683040052653\n",
      "  Batch 6,560  of  14,032.    Elapsed: 0:26:35.\n",
      "0.3287067038938403\n",
      "  Batch 6,600  of  14,032.    Elapsed: 0:26:45.\n",
      "0.30824703015387056\n",
      "  Batch 6,640  of  14,032.    Elapsed: 0:26:54.\n",
      "0.29945875592529775\n",
      "  Batch 6,680  of  14,032.    Elapsed: 0:27:04.\n",
      "0.31683000791817906\n",
      "  Batch 6,720  of  14,032.    Elapsed: 0:27:14.\n",
      "0.34991541840136053\n",
      "  Batch 6,760  of  14,032.    Elapsed: 0:27:23.\n",
      "0.3083783367648721\n",
      "  Batch 6,800  of  14,032.    Elapsed: 0:27:33.\n",
      "0.3179102336987853\n",
      "  Batch 6,840  of  14,032.    Elapsed: 0:27:43.\n",
      "0.30010318569839\n",
      "  Batch 6,880  of  14,032.    Elapsed: 0:27:53.\n",
      "0.30873951558023693\n",
      "  Batch 6,920  of  14,032.    Elapsed: 0:28:02.\n",
      "0.31952188163995743\n",
      "  Batch 6,960  of  14,032.    Elapsed: 0:28:12.\n",
      "0.33019592612981796\n",
      "  Batch 7,000  of  14,032.    Elapsed: 0:28:22.\n",
      "0.29593881871551275\n",
      "  Batch 7,056  of  14,032.    Elapsed: 0:28:35.\n",
      "0.2872152571965541\n",
      "  Batch 7,096  of  14,032.    Elapsed: 0:28:45.\n",
      "0.2858272697776556\n",
      "  Batch 7,136  of  14,032.    Elapsed: 0:28:54.\n",
      "0.3083072667941451\n",
      "  Batch 7,176  of  14,032.    Elapsed: 0:29:04.\n",
      "0.31584177669137714\n",
      "  Batch 7,216  of  14,032.    Elapsed: 0:29:14.\n",
      "0.2369557537138462\n",
      "  Batch 7,256  of  14,032.    Elapsed: 0:29:24.\n",
      "0.3180864514783025\n",
      "  Batch 7,296  of  14,032.    Elapsed: 0:29:33.\n",
      "0.336752082593739\n",
      "  Batch 7,336  of  14,032.    Elapsed: 0:29:43.\n",
      "0.2881809463724494\n",
      "  Batch 7,376  of  14,032.    Elapsed: 0:29:53.\n",
      "0.24965716917067765\n",
      "  Batch 7,416  of  14,032.    Elapsed: 0:30:03.\n",
      "0.275488924048841\n",
      "  Batch 7,456  of  14,032.    Elapsed: 0:30:12.\n",
      "0.2923286272212863\n",
      "  Batch 7,496  of  14,032.    Elapsed: 0:30:22.\n",
      "0.2721162518486381\n",
      "  Batch 7,536  of  14,032.    Elapsed: 0:30:32.\n",
      "0.2920514397323132\n",
      "  Batch 7,576  of  14,032.    Elapsed: 0:30:41.\n",
      "0.24552982449531555\n",
      "  Batch 7,616  of  14,032.    Elapsed: 0:30:51.\n",
      "0.30930235143750906\n",
      "  Batch 7,656  of  14,032.    Elapsed: 0:31:01.\n",
      "0.26516959853470323\n",
      "  Batch 7,696  of  14,032.    Elapsed: 0:31:11.\n",
      "0.28215919714421034\n",
      "  Batch 7,736  of  14,032.    Elapsed: 0:31:20.\n",
      "0.29806207716464994\n",
      "  Batch 7,776  of  14,032.    Elapsed: 0:31:30.\n",
      "0.2964428586885333\n",
      "  Batch 7,816  of  14,032.    Elapsed: 0:31:40.\n",
      "0.2627191649749875\n",
      "  Batch 7,856  of  14,032.    Elapsed: 0:31:50.\n",
      "0.3049496220424771\n",
      "  Batch 7,896  of  14,032.    Elapsed: 0:31:59.\n",
      "0.25666234754025935\n",
      "  Batch 7,936  of  14,032.    Elapsed: 0:32:09.\n",
      "0.25442893374711273\n",
      "  Batch 7,976  of  14,032.    Elapsed: 0:32:19.\n",
      "0.3106253132224083\n",
      "  Batch 8,016  of  14,032.    Elapsed: 0:32:28.\n",
      "0.2538713296875358\n",
      "  Batch 8,056  of  14,032.    Elapsed: 0:32:38.\n",
      "0.33182229921221734\n",
      "  Batch 8,096  of  14,032.    Elapsed: 0:32:48.\n",
      "0.31224956884980204\n",
      "  Batch 8,136  of  14,032.    Elapsed: 0:32:58.\n",
      "0.27221510633826257\n",
      "  Batch 8,176  of  14,032.    Elapsed: 0:33:07.\n",
      "0.29288162011653185\n",
      "  Batch 8,216  of  14,032.    Elapsed: 0:33:17.\n",
      "0.31755848713219165\n",
      "  Batch 8,256  of  14,032.    Elapsed: 0:33:27.\n",
      "0.31221764236688615\n",
      "  Batch 8,296  of  14,032.    Elapsed: 0:33:37.\n",
      "0.27735183052718637\n",
      "  Batch 8,336  of  14,032.    Elapsed: 0:33:46.\n",
      "0.2575768815353513\n",
      "  Batch 8,376  of  14,032.    Elapsed: 0:33:56.\n",
      "0.28082287330180405\n",
      "  Batch 8,416  of  14,032.    Elapsed: 0:34:06.\n",
      "0.2661721508949995\n",
      "  Batch 8,456  of  14,032.    Elapsed: 0:34:16.\n",
      "0.25224184012040496\n",
      "  Batch 8,496  of  14,032.    Elapsed: 0:34:25.\n",
      "0.2604113113135099\n",
      "  Batch 8,536  of  14,032.    Elapsed: 0:34:35.\n",
      "0.26382623203098776\n",
      "  Batch 8,576  of  14,032.    Elapsed: 0:34:45.\n",
      "0.27935868725180624\n",
      "  Batch 8,616  of  14,032.    Elapsed: 0:34:54.\n",
      "0.2621525989845395\n",
      "  Batch 8,656  of  14,032.    Elapsed: 0:35:04.\n",
      "0.27644534017890693\n",
      "  Batch 8,696  of  14,032.    Elapsed: 0:35:14.\n",
      "0.2507443930953741\n",
      "  Batch 8,736  of  14,032.    Elapsed: 0:35:24.\n",
      "0.284766286239028\n",
      "  Batch 8,776  of  14,032.    Elapsed: 0:35:33.\n",
      "0.29287538640201094\n",
      "  Batch 8,816  of  14,032.    Elapsed: 0:35:43.\n",
      "0.23906041514128445\n",
      "  Batch 8,856  of  14,032.    Elapsed: 0:35:53.\n",
      "0.3014369457960129\n",
      "  Batch 8,896  of  14,032.    Elapsed: 0:36:02.\n",
      "0.31479326970875265\n",
      "  Batch 8,936  of  14,032.    Elapsed: 0:36:12.\n",
      "0.25841741170734167\n",
      "  Batch 8,976  of  14,032.    Elapsed: 0:36:22.\n",
      "0.24283783081918955\n",
      "  Batch 9,016  of  14,032.    Elapsed: 0:36:32.\n",
      "0.2672269051894546\n",
      "  Batch 9,056  of  14,032.    Elapsed: 0:36:41.\n",
      "0.22743740696460008\n",
      "  Batch 9,096  of  14,032.    Elapsed: 0:36:51.\n",
      "0.28208905532956124\n",
      "  Batch 9,136  of  14,032.    Elapsed: 0:37:01.\n",
      "0.28304348289966585\n",
      "  Batch 9,176  of  14,032.    Elapsed: 0:37:11.\n",
      "0.2611829793080688\n",
      "  Batch 9,216  of  14,032.    Elapsed: 0:37:20.\n",
      "0.3038210799917579\n",
      "  Batch 9,256  of  14,032.    Elapsed: 0:37:30.\n",
      "0.24961292510852218\n",
      "  Batch 9,296  of  14,032.    Elapsed: 0:37:40.\n",
      "0.30808047503232955\n",
      "  Batch 9,336  of  14,032.    Elapsed: 0:37:49.\n",
      "0.24403821043670176\n",
      "  Batch 9,376  of  14,032.    Elapsed: 0:37:59.\n",
      "0.3209247801452875\n",
      "  Batch 9,416  of  14,032.    Elapsed: 0:38:09.\n",
      "0.28473436143249276\n",
      "  Batch 9,456  of  14,032.    Elapsed: 0:38:19.\n",
      "0.2639452710747719\n",
      "  Batch 9,496  of  14,032.    Elapsed: 0:38:28.\n",
      "0.2861477341502905\n",
      "  Batch 9,536  of  14,032.    Elapsed: 0:38:38.\n",
      "0.2927393139339983\n",
      "  Batch 9,576  of  14,032.    Elapsed: 0:38:48.\n",
      "0.2598263232968748\n",
      "  Batch 9,616  of  14,032.    Elapsed: 0:38:58.\n",
      "0.29103021919727323\n",
      "  Batch 9,656  of  14,032.    Elapsed: 0:39:07.\n",
      "0.2554638419300318\n",
      "  Batch 9,696  of  14,032.    Elapsed: 0:39:17.\n",
      "0.2767536548897624\n",
      "  Batch 9,736  of  14,032.    Elapsed: 0:39:27.\n",
      "0.24920497685670853\n",
      "  Batch 9,776  of  14,032.    Elapsed: 0:39:36.\n",
      "0.25596549939364194\n",
      "  Batch 9,816  of  14,032.    Elapsed: 0:39:46.\n",
      "0.2727422134950757\n",
      "  Batch 9,856  of  14,032.    Elapsed: 0:39:56.\n",
      "0.22894214456900955\n",
      "  Batch 9,896  of  14,032.    Elapsed: 0:40:06.\n",
      "0.2442554237321019\n",
      "  Batch 9,936  of  14,032.    Elapsed: 0:40:15.\n",
      "0.32712800428271294\n",
      "  Batch 9,976  of  14,032.    Elapsed: 0:40:23.\n",
      "0.3426029631868005\n",
      "  Batch 10,016  of  14,032.    Elapsed: 0:40:31.\n",
      "0.24823489245027303\n",
      "  Batch 10,056  of  14,032.    Elapsed: 0:40:41.\n",
      "0.25156942307949065\n",
      "  Batch 10,096  of  14,032.    Elapsed: 0:40:49.\n",
      "0.2607251930981874\n",
      "  Batch 10,136  of  14,032.    Elapsed: 0:40:57.\n",
      "0.27510082060471175\n",
      "  Batch 10,176  of  14,032.    Elapsed: 0:41:05.\n",
      "0.26342614982277157\n",
      "  Batch 10,216  of  14,032.    Elapsed: 0:41:13.\n",
      "0.23665739372372627\n",
      "  Batch 10,256  of  14,032.    Elapsed: 0:41:21.\n",
      "0.24781710570678114\n",
      "  Batch 10,296  of  14,032.    Elapsed: 0:41:29.\n",
      "0.2674498541280627\n",
      "  Batch 10,336  of  14,032.    Elapsed: 0:41:37.\n",
      "0.2795893717557192\n",
      "  Batch 10,376  of  14,032.    Elapsed: 0:41:46.\n",
      "0.2823126479052007\n",
      "  Batch 10,416  of  14,032.    Elapsed: 0:41:54.\n",
      "0.2651895919814706\n",
      "  Batch 10,456  of  14,032.    Elapsed: 0:42:02.\n",
      "0.27733577974140644\n",
      "  Batch 10,496  of  14,032.    Elapsed: 0:42:10.\n",
      "0.2494379824027419\n",
      "  Batch 10,536  of  14,032.    Elapsed: 0:42:18.\n",
      "0.23936576917767524\n",
      "  Batch 10,576  of  14,032.    Elapsed: 0:42:26.\n",
      "0.2607590723782778\n",
      "  Batch 10,616  of  14,032.    Elapsed: 0:42:34.\n",
      "0.2808798747137189\n",
      "  Batch 10,656  of  14,032.    Elapsed: 0:42:42.\n",
      "0.2639166234061122\n",
      "  Batch 10,696  of  14,032.    Elapsed: 0:42:50.\n",
      "0.2900145247578621\n",
      "  Batch 10,736  of  14,032.    Elapsed: 0:42:58.\n",
      "0.288526501134038\n",
      "  Batch 10,776  of  14,032.    Elapsed: 0:43:06.\n",
      "0.2719028225168586\n",
      "  Batch 10,816  of  14,032.    Elapsed: 0:43:15.\n",
      "0.2503306049853563\n",
      "  Batch 10,856  of  14,032.    Elapsed: 0:43:23.\n",
      "0.25074378717690704\n",
      "  Batch 10,896  of  14,032.    Elapsed: 0:43:31.\n",
      "0.25462740994989874\n",
      "  Batch 10,936  of  14,032.    Elapsed: 0:43:39.\n",
      "0.27762851379811765\n",
      "  Batch 10,976  of  14,032.    Elapsed: 0:43:47.\n",
      "0.24228410315699875\n",
      "  Batch 11,016  of  14,032.    Elapsed: 0:43:55.\n",
      "0.26040147626772525\n",
      "  Batch 11,056  of  14,032.    Elapsed: 0:44:03.\n",
      "0.26279699001461265\n",
      "  Batch 11,096  of  14,032.    Elapsed: 0:44:11.\n",
      "0.3012464914470911\n",
      "  Batch 11,136  of  14,032.    Elapsed: 0:44:19.\n",
      "0.2991155534982681\n",
      "  Batch 11,176  of  14,032.    Elapsed: 0:44:27.\n",
      "0.25847515016794204\n",
      "  Batch 11,216  of  14,032.    Elapsed: 0:44:35.\n",
      "0.26055399179458616\n",
      "  Batch 11,256  of  14,032.    Elapsed: 0:44:44.\n",
      "0.26625926345586776\n",
      "  Batch 11,296  of  14,032.    Elapsed: 0:44:52.\n",
      "0.26395090520381925\n",
      "  Batch 11,336  of  14,032.    Elapsed: 0:45:00.\n",
      "0.26776211578398945\n",
      "  Batch 11,376  of  14,032.    Elapsed: 0:45:08.\n",
      "0.27473999839276075\n",
      "  Batch 11,416  of  14,032.    Elapsed: 0:45:16.\n",
      "0.26036215517669914\n",
      "  Batch 11,456  of  14,032.    Elapsed: 0:45:24.\n",
      "0.24838461298495532\n",
      "  Batch 11,496  of  14,032.    Elapsed: 0:45:32.\n",
      "0.24318064376711845\n",
      "  Batch 11,536  of  14,032.    Elapsed: 0:45:40.\n",
      "0.2810427311807871\n",
      "  Batch 11,576  of  14,032.    Elapsed: 0:45:48.\n",
      "0.27600111439824104\n",
      "  Batch 11,616  of  14,032.    Elapsed: 0:45:56.\n",
      "0.24492662213742733\n",
      "  Batch 11,656  of  14,032.    Elapsed: 0:46:04.\n",
      "0.25596511662006377\n",
      "  Batch 11,696  of  14,032.    Elapsed: 0:46:13.\n",
      "0.26088650748133657\n",
      "  Batch 11,736  of  14,032.    Elapsed: 0:46:21.\n",
      "0.28857531268149617\n",
      "  Batch 11,776  of  14,032.    Elapsed: 0:46:29.\n",
      "0.27535908091813327\n",
      "  Batch 11,816  of  14,032.    Elapsed: 0:46:37.\n",
      "0.24682821799069643\n",
      "  Batch 11,856  of  14,032.    Elapsed: 0:46:45.\n",
      "0.25963080432265995\n",
      "  Batch 11,896  of  14,032.    Elapsed: 0:46:53.\n",
      "0.3081441743299365\n",
      "  Batch 11,936  of  14,032.    Elapsed: 0:47:01.\n",
      "0.24910349436104298\n",
      "  Batch 11,976  of  14,032.    Elapsed: 0:47:09.\n",
      "0.25218785665929316\n",
      "  Batch 12,016  of  14,032.    Elapsed: 0:47:17.\n",
      "0.2367126550525427\n",
      "  Batch 12,056  of  14,032.    Elapsed: 0:47:25.\n",
      "0.28141002375632523\n",
      "  Batch 12,096  of  14,032.    Elapsed: 0:47:33.\n",
      "0.21572981895878912\n",
      "  Batch 12,136  of  14,032.    Elapsed: 0:47:42.\n",
      "0.2712570745497942\n",
      "  Batch 12,176  of  14,032.    Elapsed: 0:47:50.\n",
      "0.26519759427756073\n",
      "  Batch 12,216  of  14,032.    Elapsed: 0:47:58.\n",
      "0.2300964782014489\n",
      "  Batch 12,256  of  14,032.    Elapsed: 0:48:06.\n",
      "0.2638821341097355\n",
      "  Batch 12,296  of  14,032.    Elapsed: 0:48:14.\n",
      "0.2331826371140778\n",
      "  Batch 12,336  of  14,032.    Elapsed: 0:48:22.\n",
      "0.2523865055292845\n",
      "  Batch 12,376  of  14,032.    Elapsed: 0:48:30.\n",
      "0.23758539715781807\n",
      "  Batch 12,416  of  14,032.    Elapsed: 0:48:38.\n",
      "0.27385089322924616\n",
      "  Batch 12,456  of  14,032.    Elapsed: 0:48:47.\n",
      "0.251409980840981\n",
      "  Batch 12,496  of  14,032.    Elapsed: 0:48:55.\n",
      "0.2419770418200642\n",
      "  Batch 12,536  of  14,032.    Elapsed: 0:49:04.\n",
      "0.2355778632685542\n",
      "  Batch 12,576  of  14,032.    Elapsed: 0:49:12.\n",
      "0.23153213858604432\n",
      "  Batch 12,616  of  14,032.    Elapsed: 0:49:20.\n",
      "0.22851353399455548\n",
      "  Batch 12,656  of  14,032.    Elapsed: 0:49:29.\n",
      "0.256885077804327\n",
      "  Batch 12,696  of  14,032.    Elapsed: 0:49:37.\n",
      "0.24641195740550756\n",
      "  Batch 12,736  of  14,032.    Elapsed: 0:49:46.\n",
      "0.25834895158186555\n",
      "  Batch 12,776  of  14,032.    Elapsed: 0:49:55.\n",
      "0.22173236943781377\n",
      "  Batch 12,816  of  14,032.    Elapsed: 0:50:04.\n",
      "0.2398452083580196\n",
      "  Batch 12,856  of  14,032.    Elapsed: 0:50:12.\n",
      "0.22705461792647838\n",
      "  Batch 12,896  of  14,032.    Elapsed: 0:50:20.\n",
      "0.25910038556903603\n",
      "  Batch 12,936  of  14,032.    Elapsed: 0:50:28.\n",
      "0.2602315845899284\n",
      "  Batch 12,976  of  14,032.    Elapsed: 0:50:36.\n",
      "0.2680137410759926\n",
      "  Batch 13,016  of  14,032.    Elapsed: 0:50:45.\n",
      "0.27134155835956336\n",
      "  Batch 13,056  of  14,032.    Elapsed: 0:50:53.\n",
      "0.25531612019985916\n",
      "  Batch 13,096  of  14,032.    Elapsed: 0:51:02.\n",
      "0.26031304989010096\n",
      "  Batch 13,136  of  14,032.    Elapsed: 0:51:11.\n",
      "0.24556121416389942\n",
      "  Batch 13,176  of  14,032.    Elapsed: 0:51:19.\n",
      "0.2607356941327453\n",
      "  Batch 13,216  of  14,032.    Elapsed: 0:51:28.\n",
      "0.22500000782310964\n",
      "  Batch 13,256  of  14,032.    Elapsed: 0:51:37.\n",
      "0.2669368989765644\n",
      "  Batch 13,296  of  14,032.    Elapsed: 0:51:45.\n",
      "0.2789370337501168\n",
      "  Batch 13,336  of  14,032.    Elapsed: 0:51:53.\n",
      "0.2737539170309901\n",
      "  Batch 13,376  of  14,032.    Elapsed: 0:52:01.\n",
      "0.27761075720191003\n",
      "  Batch 13,416  of  14,032.    Elapsed: 0:52:09.\n",
      "0.26766645181924104\n",
      "  Batch 13,456  of  14,032.    Elapsed: 0:52:18.\n",
      "0.25318988487124444\n",
      "  Batch 13,496  of  14,032.    Elapsed: 0:52:26.\n",
      "0.2561828013509512\n",
      "  Batch 13,536  of  14,032.    Elapsed: 0:52:34.\n",
      "0.23747178269550204\n",
      "  Batch 13,576  of  14,032.    Elapsed: 0:52:42.\n",
      "0.28018386047333477\n",
      "  Batch 13,616  of  14,032.    Elapsed: 0:52:51.\n",
      "0.22610302940011023\n",
      "  Batch 13,656  of  14,032.    Elapsed: 0:53:00.\n",
      "0.3284008979797363\n",
      "  Batch 13,696  of  14,032.    Elapsed: 0:53:08.\n",
      "0.2523076839745045\n",
      "  Batch 13,736  of  14,032.    Elapsed: 0:53:17.\n",
      "0.23774281442165374\n",
      "  Batch 13,776  of  14,032.    Elapsed: 0:53:25.\n",
      "0.25298323165625336\n",
      "  Batch 13,816  of  14,032.    Elapsed: 0:53:33.\n",
      "0.24376392364501953\n",
      "  Batch 13,856  of  14,032.    Elapsed: 0:53:41.\n",
      "0.22946832040324808\n",
      "  Batch 13,896  of  14,032.    Elapsed: 0:53:50.\n",
      "0.22907958328723907\n",
      "  Batch 13,936  of  14,032.    Elapsed: 0:53:59.\n",
      "0.2598637825809419\n",
      "  Batch 13,976  of  14,032.    Elapsed: 0:54:07.\n",
      "0.2359159529209137\n",
      "  Batch 14,016  of  14,032.    Elapsed: 0:54:16.\n",
      "0.2442428521811962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.22595343738794327, 0.915)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(model,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):    \n",
    "    avg_acc = []    \n",
    "    model.eval()         #表示进入测试模式\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_id,input_mask,label,valid_num_tensor = batch[0].long().to(device),batch[1].long().to(device),batch[2].long().to(device),batch[3]\n",
    "            valid_num = valid_num_tensor.tolist()\n",
    "            output = model(input_id,input_mask,valid_num)\n",
    "            acc = binary_acc_eva(output[1], label,8,valid_num)\n",
    "            avg_acc.append(acc)\n",
    "    avg_acc = np.array(avg_acc).mean()\n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1,测试准确率=0.7721593519556661\n"
     ]
    }
   ],
   "source": [
    "test_acc = evaluate(model)\n",
    "print(\"epoch={},测试准确率={}\".format(1, test_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
