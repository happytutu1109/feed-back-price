{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler \n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数目： 534\n"
     ]
    }
   ],
   "source": [
    "def data_loading(data_dir):\n",
    "    data, labels, ids= [], [], []\n",
    "    df = pd.read_csv(data_dir).loc[:5000]\n",
    "    data_list = df.loc[:,'discourse_text'].values\n",
    "    label_list_ = df.loc[:,'discourse_type'].values\n",
    "    id_list = df.loc[:,'id'].values\n",
    "    type_dict = {'Lead':1,'Position':2,'Claim':3,'Counterclaim':4,'Rebuttal':5,'Evidence':6,'Concluding Statement':7}\n",
    "    label_list = []\n",
    "    for i in label_list_:\n",
    "        label_list.append(type_dict[i])\n",
    "    if len(data_list)!=len(label_list):\n",
    "        return 'length bug'\n",
    "    n = len(data_list)\n",
    "    for data_ in range(n):\n",
    "        sentence_split = data_list[data_].split(\".\")[:-1] if data_list[data_].split('.')[-1] == str('') else data_list[data_].split(\".\")\n",
    "        label_split = [label_list[data_] for i in range(len(sentence_split))]\n",
    "        id_split = [id_list[data_] for i in range(len(sentence_split))]\n",
    "        for j in range(len(sentence_split)):\n",
    "            if sentence_split[j]!=' ':\n",
    "                data.append(sentence_split[j].lower()) \n",
    "                labels.append(label_split[j])\n",
    "                ids.append(id_split[j])\n",
    "    data_article, labels_article = [],[]\n",
    "    data_sentence, labels_sentence = [],[]\n",
    "    for i in range(len(ids)-1):\n",
    "        if ids[i]==ids[i+1]: \n",
    "            data_sentence.append(data[i])\n",
    "            labels_sentence.append(labels[i])\n",
    "        else:\n",
    "            data_article.append(data_sentence)\n",
    "            labels_article.append(labels_sentence)\n",
    "            data_sentence,labels_sentence = [],[]\n",
    "    return data_article , labels_article\n",
    "\n",
    "\n",
    "train_data = data_loading('./train.csv')\n",
    "print('训练集数目：', len(train_data[0]))\n",
    "#print(train_data[0][0],train_data[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAARlklEQVR4nO3df6zddX3H8edLUDb8MUEupALdBVPd0GjRm07HNCj+QDSiS3Ql0+BGVk0g081kFk38lZCwzR9bsompg8E2raKIEmEqY07jMsUWKhZKBaRipWuvsImbhtn63h/n23C83Ou9PeeentMPz0dycr7fz/l+z/fV28urXz7ne85JVSFJasujxh1AkrT8LHdJapDlLkkNstwlqUGWuyQ1yHKXpAYtWu5JTkzy5STbktya5C3d+NFJrk9yR3d/VN8+Fya5M8n2JC8b5R9AkvRwWew69yQrgBVVdVOSxwObgVcDbwTur6qLk6wHjqqqtyc5BdgIrAGeDPwL8NSq2je6P4Ykqd+iZ+5VtauqbuqWfwxsA44Hzgau6Da7gl7h041/oqoerKq7gTvpFb0k6SA5/EA2TjINnAp8AziuqnZB7x+AJMd2mx0PfL1vt53d2IKOOeaYmp6ePpAokvSIt3nz5h9W1dR8jy253JM8DrgKeGtVPZBkwU3nGXvY3E+SdcA6gJUrV7Jp06alRpEkAUm+t9BjS7paJsmj6RX7x6rqM93w7m4+fv+8/J5ufCdwYt/uJwD3zn3OqtpQVTNVNTM1Ne8/PJKkAS3lapkAlwLbquqDfQ9dA5zbLZ8LfK5vfG2SI5KcBKwCbly+yJKkxSxlWuY04A3At5Ns6cbeAVwMXJnkPOAe4LUAVXVrkiuB24C9wPleKSNJB9ei5V5VX2P+eXSAMxbY5yLgoiFySZKG4DtUJalBlrskNchyl6QGWe6S1CDLXZIadEAfP6DJML3+2rEde8fFrxjbsSUtnWfuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWrQUr4g+7Ike5Js7Rv7ZJIt3W3H/u9WTTKd5Kd9j31khNklSQtYyqdCXg78DfAP+weq6vf2Lyf5APCjvu3vqqrVy5RPkjSApXxB9leTTM/3WJIArwNetMy5JElDGHbO/fnA7qq6o2/spCQ3J/lKkucP+fySpAEM+2Ud5wAb+9Z3ASur6r4kzwE+m+TpVfXA3B2TrAPWAaxcuXLIGJKkfgOfuSc5HPhd4JP7x6rqwaq6r1veDNwFPHW+/atqQ1XNVNXM1NTUoDEkSfMYZlrmxcDtVbVz/0CSqSSHdcsnA6uA7w4XUZJ0oJZyKeRG4D+ApyXZmeS87qG1/OKUDMALgFuSfAv4NPDmqrp/OQNLkha3lKtlzllg/I3zjF0FXDV8LEnSMHyHqiQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBi3lO1QvS7Inyda+sfck+UGSLd3trL7HLkxyZ5LtSV42quCSpIUt5cz9cuDMecY/VFWru9t1AElOoffF2U/v9vlwksOWK6wkaWkWLfeq+ipw/xKf72zgE1X1YFXdDdwJrBkinyRpAMPMuV+Q5JZu2uaobux44Pt92+zsxiRJB9Gg5X4J8BRgNbAL+EA3nnm2rfmeIMm6JJuSbJqdnR0whiRpPgOVe1Xtrqp9VfVz4KM8NPWyEzixb9MTgHsXeI4NVTVTVTNTU1ODxJAkLWCgck+yom/1NcD+K2muAdYmOSLJScAq4MbhIkqSDtThi22QZCNwOnBMkp3Au4HTk6ymN+WyA3gTQFXdmuRK4DZgL3B+Ve0bSXJJ0oIWLfeqOmee4Ut/yfYXARcNE0qSNJxFy13qN73+2rEcd8fFrxjLcaVDlR8/IEkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQYuWe5LLkuxJsrVv7C+T3J7kliRXJ3liNz6d5KdJtnS3j4wwuyRpAUs5c78cOHPO2PXAM6rqmcB3gAv7HrurqlZ3tzcvT0xJ0oFYtNyr6qvA/XPGvlRVe7vVrwMnjCCbJGlAyzHn/ofAP/etn5Tk5iRfSfL8hXZKsi7JpiSbZmdnlyGGJGm/oco9yTuBvcDHuqFdwMqqOhX4U+DjSZ4w375VtaGqZqpqZmpqapgYkqQ5Bi73JOcCrwR+v6oKoKoerKr7uuXNwF3AU5cjqCRp6QYq9yRnAm8HXlVVP+kbn0pyWLd8MrAK+O5yBJUkLd3hi22QZCNwOnBMkp3Au+ldHXMEcH0SgK93V8a8AHhfkr3APuDNVXX/vE8sSRqZRcu9qs6ZZ/jSBba9Crhq2FCSpOH4DlVJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWrQou9Q1cKm11877giSNC/P3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNWrTck1yWZE+SrX1jRye5Pskd3f1RfY9dmOTOJNuTvGxUwSVJC1vKmfvlwJlzxtYDN1TVKuCGbp0kpwBrgad3+3w4yWHLllaStCSLlntVfRW4f87w2cAV3fIVwKv7xj9RVQ9W1d3AncCa5YkqSVqqQefcj6uqXQDd/bHd+PHA9/u229mNPUySdUk2Jdk0Ozs7YAxJ0nyW+wXVzDNW821YVRuqaqaqZqamppY5hiQ9sg1a7ruTrADo7vd04zuBE/u2OwG4d/B4kqRBDFru1wDndsvnAp/rG1+b5IgkJwGrgBuHiyhJOlCLfp57ko3A6cAxSXYC7wYuBq5Mch5wD/BagKq6NcmVwG3AXuD8qto3ouySpAUsWu5Vdc4CD52xwPYXARcNE0qSNBzfoSpJDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGLfs3eQpI8Dfhk39DJwLuAJwJ/BMx24++oqusGPY4k6cANXO5VtR1YDZDkMOAHwNXAHwAfqqr3L0dASdKBW65pmTOAu6rqe8v0fJKkISxXua8FNvatX5DkliSXJTlqvh2SrEuyKcmm2dnZ+TaRJA1o6HJP8hjgVcCnuqFLgKfQm7LZBXxgvv2qakNVzVTVzNTU1LAxJEl9luPM/eXATVW1G6CqdlfVvqr6OfBRYM0yHEOSdACWo9zPoW9KJsmKvsdeA2xdhmNIkg7AwFfLACQ5EngJ8Ka+4b9IshooYMecxyRJB8FQ5V5VPwGeNGfsDUMlkiQNzXeoSlKDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lq0LDfoboD+DGwD9hbVTNJjgY+CUzT+w7V11XVfw0XU5J0IJbjzP2FVbW6qma69fXADVW1CrihW5ckHUSjmJY5G7iiW74CePUIjiFJ+iWGLfcCvpRkc5J13dhxVbULoLs/dshjSJIO0FBz7sBpVXVvkmOB65PcvtQdu38M1gGsXLlyyBiSpH5DnblX1b3d/R7gamANsDvJCoDufs8C+26oqpmqmpmamhomhiRpjoHP3JM8FnhUVf24W34p8D7gGuBc4OLu/nPLEVSPbNPrrx3bsXdc/IqxHVsa1DDTMscBVyfZ/zwfr6ovJPkmcGWS84B7gNcOH1OSdCAGLveq+i7wrHnG7wPOGCaUJGk4vkNVkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDBi73JCcm+XKSbUluTfKWbvw9SX6QZEt3O2v54kqSlmKYL8jeC7ytqm5K8nhgc5Lru8c+VFXvHz6eJGkQw3xB9i5gV7f84yTbgOOXK5gkaXDLMueeZBo4FfhGN3RBkluSXJbkqOU4hiRp6YYu9ySPA64C3lpVDwCXAE8BVtM7s//AAvutS7IpyabZ2dlhY0iS+gxV7kkeTa/YP1ZVnwGoqt1Vta+qfg58FFgz375VtaGqZqpqZmpqapgYkqQ5hrlaJsClwLaq+mDf+Iq+zV4DbB08niRpEMNcLXMa8Abg20m2dGPvAM5JshooYAfwpiGOIUkawDBXy3wNyDwPXTd4HEnScvAdqpLUIMtdkhpkuUtSg4Z5QVV6RJhef+1Yjrvj4leM5bhqg2fuktQgy12SGmS5S1KDmphzH9ecqCRNKs/cJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoOauBRSatE4L/H1ow8OfZ65S1KDLHdJapDlLkkNGlm5JzkzyfYkdyZZP6rjSJIebiTlnuQw4G+BlwOn0PvS7FNGcSxJ0sON6sx9DXBnVX23qv4P+ARw9oiOJUmaY1Tlfjzw/b71nd2YJOkgGNV17plnrH5hg2QdsK5b/Z8k2w/g+Y8BfjhgtlGa1FwwudkmNRdMbraR58qfD7zrI/ZnNoRhsv36Qg+Mqtx3Aif2rZ8A3Nu/QVVtADYM8uRJNlXVzODxRmNSc8HkZpvUXDC52SY1F0xutknNBaPLNqppmW8Cq5KclOQxwFrgmhEdS5I0x0jO3Ktqb5ILgC8ChwGXVdWtoziWJOnhRvbZMlV1HXDdiJ5+oOmcg2BSc8HkZpvUXDC52SY1F0xutknNBSPKlqpafCtJ0iHFjx+QpAYdUuU+SR9pkOSyJHuSbO0bOzrJ9Unu6O6PGkOuE5N8Ocm2JLcmecsEZfuVJDcm+VaX7b2Tkq3LcViSm5N8fsJy7Ujy7SRbkmyalGxJnpjk00lu737fnjchuZ7W/az23x5I8tYJyfYn3e/+1iQbu/8mRpLrkCn3CfxIg8uBM+eMrQduqKpVwA3d+sG2F3hbVf0m8Fzg/O7nNAnZHgReVFXPAlYDZyZ57oRkA3gLsK1vfVJyAbywqlb3XTI3Cdn+GvhCVf0G8Cx6P7ux56qq7d3PajXwHOAnwNXjzpbkeOCPgZmqega9i03WjixXVR0SN+B5wBf71i8ELhxzpmlga9/6dmBFt7wC2D4BP7fPAS+ZtGzAkcBNwG9NQjZ678W4AXgR8PlJ+vsEdgDHzBkbazbgCcDddK/bTUqueXK+FPj3ScjGQ+/cP5rexSyf7/KNJNchc+bOofGRBsdV1S6A7v7YcYZJMg2cCnyDCcnWTX1sAfYA11fVpGT7K+DPgJ/3jU1CLui9u/tLSTZ37+yehGwnA7PA33dTWX+X5LETkGuutcDGbnms2arqB8D7gXuAXcCPqupLo8p1KJX7oh9poIckeRxwFfDWqnpg3Hn2q6p91fvf5ROANUmeMeZIJHklsKeqNo87ywJOq6pn05uSPD/JC8YdiN6Z57OBS6rqVOB/Ge+01cN0b6B8FfCpcWcB6ObSzwZOAp4MPDbJ60d1vEOp3Bf9SIMJsDvJCoDufs84QiR5NL1i/1hVfWaSsu1XVf8N/Bu91y3Gne004FVJdtD7BNMXJfmnCcgFQFXd293voTd3vGYCsu0Ednb/5wXwaXplP+5c/V4O3FRVu7v1cWd7MXB3Vc1W1c+AzwC/Papch1K5HwofaXANcG63fC69+e6DKkmAS4FtVfXBCcs2leSJ3fKv0vtlv33c2arqwqo6oaqm6f1e/WtVvX7cuQCSPDbJ4/cv05uj3TrubFX1n8D3kzytGzoDuG3cueY4h4emZGD82e4BnpvkyO6/0zPovQg9mlzjfLFjgBckzgK+A9wFvHPMWTbSmzf7Gb2zmPOAJ9F7Ue6O7v7oMeT6HXrTVbcAW7rbWROS7ZnAzV22rcC7uvGxZ+vLeDoPvaA69lz05ra/1d1u3f97PyHZVgObur/PzwJHTUKuLtuRwH3Ar/WNjT0b8F56JzRbgX8EjhhVLt+hKkkNOpSmZSRJS2S5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUoP8HXS5hW2RFR/wAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mlt\n",
    "plt.hist([len(train_data[1][i]) for i in range(len(train_data[1]))],bins = 10,rwidth=1, range=(1,80))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 1, 1, 2, 6, 6, 6, 6, 6, 6, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 7], [2, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7], [1, 1, 1, 2, 3, 6, 6, 6, 4, 4, 5, 5, 7], [1, 1, 1, 2, 3, 3, 3, 3, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7], [1, 1, 1, 1, 2, 2, 3, 6, 6, 6, 6, 6, 3, 6, 6, 6, 6, 6, 6, 6, 6, 3, 6, 6, 6, 7]]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "article,labels_ = train_data[0],train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2226: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [tensor([[  101,  2715,  4286,  2651,  2024,  2467,  2006,  2037,  3042,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2024,  2467,  2006,  2037,  3042,  2062,  2084,  1019,\n",
      "          2847,  1037,  2154,  2053,  2644,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2035,  2027,  2079,  2003,  3793,  2067,  1998,  2830,  1998,\n",
      "          2074,  2031,  2177, 11834,  2015,  2006,  2591,  2865,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2130,  2079,  2009,  2096,  4439,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2027,  2024,  2070,  2428,  2919,  8465,  2043,  4933,  6433,\n",
      "          2043,  2009,  3310,  2000,  1037,  3042,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2070,  3056,  2752,  1999,  1996,  2142,  2163,  7221, 11640,\n",
      "          2013,  2465,  4734,  2074,  2138,  1997,  2009,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2043,  2111,  2031, 11640,  1010,  2027,  2113,  2055,  3056,\n",
      "         18726,  2008,  2027,  2031,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 18726,  2066,  9130, 10474, 16021, 23091,  1998, 10245,  7507,\n",
      "          2102,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2061,  2066,  2065,  1037,  2767,  5829,  2185,  1998,  2017,\n",
      "          2215,  2000,  2022,  1999,  3967,  2017,  2064,  2145,  2022,  1999,\n",
      "          3967,  2011, 14739,  6876,  2030,  3793,  7696,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2111,  2467,  2031,  2367,  3971,  2129,  2000, 10639,  2007,\n",
      "          1037,  3042,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 11640,  2031,  2904,  2349,  2000,  2256,  4245,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  4439,  2003,  2028,  1997,  1996,  2126,  2129,  2000,  2131,\n",
      "          2105,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2111,  2467,  2022,  2006,  2037, 11640,  2096,  2725,  2009,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2029,  2064,  3426,  3809,  3471,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2008,  1005,  1055,  2339,  2045,  1005,  1055,  1037,  2518,\n",
      "          2008,  1005,  1055,  2170,  2053,  3793,  2075,  2096,  4439,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2008,  1005,  1055,  1037,  2428,  2590,  2518,  2000,  3342,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2070,  2111,  2145,  2079,  2009,  2138,  2027,  2228,  2009,\n",
      "          1005,  1055,  5236,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2053,  3043,  2054,  2027,  2079,  2027,  2145,  2031,  2000,\n",
      "         15470,  2009,  2138,  2008,  1005,  1055,  1996,  2069,  2126,  2129,\n",
      "          2106,  2002,  3828,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2823,  2006,  1996,  2739,  2045,  2003,  2593,  2019,  4926,\n",
      "          2030,  1037,  5920,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2009,  2453,  9125,  2619,  2025,  2559,  2073,  2027,  1005,\n",
      "          2128,  2183,  2030,  1056, 28394,  2102,  2008,  2619,  2741,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2009,  2593,  4544,  2030,  2331,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2065,  1037,  8075,  2193,  2758,  1045,  1005,  1049,  2183,\n",
      "          2000,  3102,  2017,  1998,  2027,  2113,  2073,  2017,  2444,  2021,\n",
      "          2017,  2123,  1005,  1056,  2113,  1996,  2711,  1005,  1055,  3967,\n",
      "          1010,   102],\n",
      "        [  101,  2029,  2064,  2203,  2039,  2428,  6649,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 11640,  2024,  2986,  2000,  2224,  1998,  2009,  1005,  1055,\n",
      "          2036,  1996,  2190,  2126,  2000,  2272,  2058,  2393,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2065,  2017,  2175,  2083,  1037,  3291,  1998,  2017,  2064,\n",
      "          1005,  1056,  2424,  2393,  2017,  1010,  2467,  2031,  1037,  3042,\n",
      "          2045,  2007,  2017,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2130,  2295, 11640,  2024,  2109,  2471,  2296,  2154,  2004,\n",
      "          2146,  2004,  2017,  1005,  2128,  3647,  2009,  2052,  2272,  2046,\n",
      "          2224,  2065,  2017,  2131,  2046,  4390,   102,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2191,  2469,  2017,  2079,  2025,  2022,  2066,  2023,  3042,\n",
      "          2096,  2017,  1005,  2128,  1999,  1996,  2690,  1997,  4439,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1996,  2739,  2467,  7172,  2043,  2111,  2079,  2242,  5236,\n",
      "          2105,  2008,  7336,  2037, 11640,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), tensor([[  101,  6853,  2323,  2025,  2022,  2583,  2000,  2224, 11640,  2096,\n",
      "          4082,  1037,  4316,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2040,  2109,  2037,  3042,  2096,  4082,  1037,  4316,\n",
      "          2024,  2087,  3497,  2000,  2131,  2046,  2019,  4926,  2008,  2071,\n",
      "          2022, 10611,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  2019,  3720,  2011,  1996,  9586, 17840,  3813,\n",
      "          1010,  2538,  1003,  1997, 13496,  2008,  2020,  2112,  1997,  1037,\n",
      "         10611,  2482,  4926,  2001,  2349,  2000, 11640,   102,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  1996,  2168,  3720,  1010,  3486,  1003,  2113,\n",
      "          1996,  3891,  2021,  3613,  2478,  2037, 11640,  2096,  2006,  1996,\n",
      "          2346,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2023,  3065,  2008,  2049,  3458,  4795,  1998, 20868,  6072,\n",
      "         26029, 19307,  1997,  6853,  2025,  2000,  2022,  3929,  5204,  1997,\n",
      "          2037, 11301,  2096,  4439,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2323,  2022,  2583,  2000, 10152,  2302,  2151, 14836,\n",
      "          2015,  1010,  2138,  2009,  2071,  2022, 10611,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  2429,  2000,  2178,  3720,  1010,  1000, 11116,  4439,  1000,\n",
      "          2011,  1996, 18699, 27110,  1010,  2045,  2038,  2525,  2042,  2055,\n",
      "          1017,  1010,  2199,  3042,  3141,  2482,  4926,  6677,  2144,  2418,\n",
      "           102,     0],\n",
      "        [  101,  1996,  3720,  2163,  2008,  9458,  2131,  2205, 11116,  2007,\n",
      "          2037, 11640,  1010,  2029,  5320,  2037,  4926,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101, 13436,  2008,  2064,  2022,  4089,  9511,  2011,  7995,  2006,\n",
      "          1996,  2346,  1998,  2025,  1037,  3042,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2323,  2025,  2022,  2583,  2000,  2224,  2037, 11640,\n",
      "          2012,  2035,  2096,  4439,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  1999,  7091,  1010,  6853,  2323,  2025,  2583,  2000,  2147,\n",
      "          1037,  4316,  2096,  2478,  2037,  3526,  3042,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  101,  6853,  2040,  3594,  2037, 11640,  2096,  4082,  1037,  4316,\n",
      "          1998,  2024,  3497,  2000,  2031,  2019,  4926,  2059,  2216,  2040,\n",
      "          2123,  1005,  1056,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])] [tensor([1, 1, 1, 1, 2, 6, 6, 6, 6, 6, 6, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 3,\n",
      "        6, 6, 6, 7]), tensor([2, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7])]\n",
      "[tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "input_ids_art = []\n",
    "attention_masks_art = []\n",
    "labels = []\n",
    "# For every sentence...\n",
    "for art in range(len(article)):\n",
    "    input_ids_sent = []\n",
    "    attention_masks_sent = []\n",
    "    for sent in article[art]:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sent,                      # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = 32,           # Pad & truncate all sentences.\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,   # Construct attn. masks.\n",
    "                            return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                    )\n",
    "        \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids_sent.append(encoded_dict['input_ids'])\n",
    "        \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks_sent.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "    input_ids_sent = torch.cat(input_ids_sent, dim=0)\n",
    "    attention_masks_sent = torch.cat(attention_masks_sent, dim=0)\n",
    "    labels_sent = torch.tensor(labels_[art])\n",
    "\n",
    "    input_ids_art.append(input_ids_sent)\n",
    "    attention_masks_art.append(attention_masks_sent)\n",
    "    labels.append(labels_sent)\n",
    "\n",
    "#labels = [label.tolist() for label in labels]\n",
    "\n",
    "#print('Original: ', article[0])\n",
    "print('Token IDs:', input_ids_art[:2],labels[:2])\n",
    "print(attention_masks_art[:2])\n",
    "#print(labels[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class article_dataset(Dataset):\n",
    "    def __init__(self,input,target,masks):\n",
    "        super(article_dataset,self).__init__()\n",
    "        self.input = input\n",
    "        self.target = target\n",
    "        self.masks = masks\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input[idx],self.masks[idx],self.target[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章中句子个数不一致问题暂定解决办法：\n",
    "    1：找到合适的sentence_num（尽量所有文章的长度均小于该值）。\n",
    "    2：对小于该长度的文章，在处理完tokenizer之前进行空句子补齐，并记录其有效长度。（其会在tokennizer阶段被标注为[101,102,...,0]）。\n",
    "    3：所有用于补齐的空句子不设置类别，在处理空句子时记录每篇文章的有效句子长度。句子有效长度在每一个batch中使用一个list储存（想办法在dataset中实现）\n",
    "    4：在模型框架中，仅输出有效句子的损失并进行后向传播。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1a81de22280>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = article_dataset(input_ids_art,labels,attention_masks_art)\n",
    "dataloder = DataLoader(dataset,batch_size=5,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self,bert_name,hidden_size_rnn,num_layers_rnn,classes,dropout):     #valid_len表示该input中有效的sentence个数\n",
    "        super(HAN,self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_name)\n",
    "        hidden_size_bert = self.bert.config.hidden_size\n",
    "        self.rnn = nn.LSTM(input_size=hidden_size_bert, hidden_size=hidden_size_rnn, num_layers=num_layers_rnn, bias=True, dropout=dropout,batch_first=True)\n",
    "        self.outlayer = nn.Linear(hidden_size_rnn,classes)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.classes = classes\n",
    "        \n",
    "    def forward(self,input_ids,input_masks,valid_len,labels=None):  #input维度 batchsize,num_sentence,sent_len , label维度batchsize,num_sentence,sent_len\n",
    "        batch_size,sentence_num,sentence_len = input_ids.size(0),input_ids.size(1),input_ids.size(2)\n",
    "        hidden_size_bert = self.bert.config.hidden_size\n",
    "        input_ids,input_masks = input_ids.view(-1,sentence_len),input_masks.view(-1,sentence_len)\n",
    "        out = self.bert(input_ids,input_masks)      #out维度 batchsize*num_sentence,sen_len,hidden_size\n",
    "        out = out[0][:,0,:]     #out维度 batchsize*num_sentence,hidden_size\n",
    "        #下面这行有可能会报错！！！\n",
    "        out = out.view(batch_size,sentence_num,hidden_size_bert)    #out维度转化为batchsize,num_sentence,hidden_size\n",
    "        self.rnn.flatten_parameters()\n",
    "        outputs,_ = self.rnn(out)   #outputs维度 batchsize,num_sentence,hidden_size\n",
    "        outputs = self.outlayer(outputs)    #outputs维度为 batchsize,num_sentence,7\n",
    "        output = []\n",
    "        for batch in range(len(outputs)):\n",
    "            output.append(outputs[batch][0:valid_len[batch],:].tolist())\n",
    "        if labels is not None:\n",
    "            loss_output = []\n",
    "            for i in output:\n",
    "                for j in i:\n",
    "                    loss_output.append(j)\n",
    "            loss_labels = []\n",
    "            for k in labels:\n",
    "                for l in k:\n",
    "                    loss_labels.append(l)\n",
    "            loss = self.loss(torch.tensor(loss_output),torch.tensor(loss_labels))\n",
    "            return loss,outputs\n",
    "        else:\n",
    "            return 0,outputs\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "        [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "         1, 1, 1, 1, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = torch.tensor([[[  101,  2715,  4286,  2651,  2024,  2467,  2006,  2037,  3042,   102,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2027,  2024,  2467,  2006,  2037,  3042,  2062,  2084,  1019,\n",
    "          2847,  1037,  2154,  2053,  2644,   102,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]],\n",
    "     [[  101,  2035,  2027,  2079,  2003,  3793,  2067,  1998,  2830,  1998,\n",
    "          2074,  2031,  2177, 11834,  2015,  2006,  2591,  2865,   102,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2027,  2130,  2079,  2009,  2096,  4439,   102,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2027,  2024,  2070,  2428,  2919,  8465,  2043,  4933,  6433,\n",
    "          2043,  2009,  3310,  2000,  1037,  3042,   102,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "          [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "     [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]],\n",
    "     [[  101,  2070,  3056,  2752,  1999,  1996,  2142,  2163,  7221, 11640,\n",
    "          2013,  2465,  4734,  2074,  2138,  1997,  2009,   102,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2043,  2111,  2031, 11640,  1010,  2027,  2113,  2055,  3056,\n",
    "         18726,  2008,  2027,  2031,   102,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101, 18726,  2066,  9130, 10474, 16021, 23091,  1998, 10245,  7507,\n",
    "          2102,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2061,  2066,  2065,  1037,  2767,  5829,  2185,  1998,  2017,\n",
    "          2215,  2000,  2022,  1999,  3967,  2017,  2064,  2145,  2022,  1999,\n",
    "          3967,  2011, 14739,  6876,  2030,  3793,  7696,   102,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,  2111,  2467,  2031,  2367,  3971,  2129,  2000, 10639,  2007,\n",
    "          1037,  3042,   102,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0],\n",
    "        [  101,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
    "             0,     0]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id = sentence\n",
    "input_mask = mask\n",
    "labels = torch.tensor([[2,3,0,0,0,0],[5,3,2,0,0,0],[5,5,5,2,6,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(1.9443), tensor([[[ 0.0065, -0.0253, -0.0229,  0.0220, -0.0272, -0.0076, -0.0156],\n",
      "         [ 0.0135, -0.0305, -0.0271,  0.0293, -0.0308, -0.0082, -0.0203],\n",
      "         [ 0.0082, -0.0228, -0.0206,  0.0300, -0.0287, -0.0163, -0.0117],\n",
      "         [ 0.0044, -0.0177, -0.0077,  0.0377, -0.0236, -0.0176, -0.0088],\n",
      "         [-0.0058, -0.0121, -0.0002,  0.0362, -0.0278, -0.0175, -0.0076],\n",
      "         [-0.0045, -0.0149,  0.0025,  0.0388, -0.0260, -0.0227, -0.0032]],\n",
      "\n",
      "        [[-0.0003, -0.0308, -0.0237,  0.0228, -0.0275, -0.0110, -0.0130],\n",
      "         [ 0.0086, -0.0332, -0.0306,  0.0235, -0.0263, -0.0092, -0.0180],\n",
      "         [ 0.0155, -0.0344, -0.0310,  0.0201, -0.0234, -0.0093, -0.0175],\n",
      "         [ 0.0145, -0.0270, -0.0201,  0.0195, -0.0278, -0.0145, -0.0137],\n",
      "         [ 0.0021, -0.0224, -0.0057,  0.0252, -0.0273, -0.0116, -0.0151],\n",
      "         [-0.0040, -0.0193,  0.0030,  0.0293, -0.0294, -0.0218, -0.0048]],\n",
      "\n",
      "        [[ 0.0043, -0.0240, -0.0270,  0.0223, -0.0277, -0.0081, -0.0131],\n",
      "         [ 0.0170, -0.0296, -0.0390,  0.0221, -0.0254, -0.0095, -0.0124],\n",
      "         [ 0.0171, -0.0277, -0.0414,  0.0243, -0.0203, -0.0128, -0.0085],\n",
      "         [ 0.0225, -0.0315, -0.0425,  0.0245, -0.0226, -0.0125, -0.0055],\n",
      "         [ 0.0247, -0.0364, -0.0448,  0.0240, -0.0289, -0.0097, -0.0045],\n",
      "         [ 0.0167, -0.0333, -0.0296,  0.0272, -0.0228, -0.0096,  0.0004]]],\n",
      "       grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "model = HAN('bert-base-uncased',1024,2,7,0.1)\n",
    "output = model(input_id,input_mask,[2,3,5],labels=[[2,3],[5,3,2],[5,5,5,2,6]])\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
